Парная регрессия

$\mathsf{y = \overset{\wedge}f(x)}$

  

$\mathsf y$ - значимая * результат *

$\mathsf x$ - независимая (объясняющая) переменная (признак-фактора)

  

$\mathsf{y = a + bx + \epsilon}$

  

==**Нелинейная регрессия**==

1. Нелинейная относительно включённых в анализ объясняющих переменных, но линейных по оцениваемым параметрам
2. Нелинейная по оцениваемым параметрам

  

Регрессии, нелинейные по объясняющим переменным:

1. Полиномы
    1. $\mathsf{ y = a + b_1x + b_2x^2 + b_3 x^3 + \epsilon }$
    2. $\mathsf{ y = a + \dfrac b x + \epsilon }$
2. Нелинейные по оцениваемым параметрам
    1. $\mathsf{ y = a \cdot x^b + \epsilon }$
    2. b
        1. $\mathsf{ y = ab^x+\epsilon }$
    3. Экспоненциальная
        1. $\mathsf{ y = e^{a + bx} + \epsilon }$

  

Построение уравнения регрессии сводится к оценке её параметров

Для оценки параметров уравнения регрессий линейных по параметрам используют **Метод Квадратов Наименьших**

$\mathsf{ \sum (y - \overset{\wedge}y_x)^2 \to \min }$

y - фактич.

y^ - теоретич.

  

Для линейных и нелинейных уравнений, приводимых к линейным, решается система, относительно $\mathsf a$ и $\mathsf b$:

$\begin{cases} \mathsf{a \sum1 + b \sum x = \sum y} \\ \mathsf{a \sum x + b \sum x^2 = \sum yx} \end{cases}$

$\mathsf{ a = \overline y - b \overline x }$

$\mathsf{ b = \dfrac{cov (x, y)}{\sigma_x^2} = \dfrac{\overline{yx} - \overline y \cdot \overline x}{\overline{x^2} - \overline x^2} }$

  

Тесноту связи изучаемых явлений изучает линейный коэффициент парной кореляции

Для линейной регрессии: $\mathsf{ (-1 \le r_{xy} \le 1 )}$

$\mathsf{ r_{xy} = b \dfrac{\sigma_x}{\sigma_y} = \dfrac{cov(x, y)}{\sigma_x \sigma_y} = \dfrac{\overline{yx} - \overline y \cdot \overline x}{\sigma_x \sigma_y} }$

  

Коэффициент парной кореляции $\mathsf{ (0 \le \rho_{xy} \le 1) }$

$\mathsf{ \rho_{xy} = \sqrt{1 - \dfrac{\sigma^2_{ост}}{\sigma^2_y}} = \sqrt{1 - \dfrac{\sum (y - \overset{\wedge}y_x)^2}{\sum(y - \overline y)^2}} }$

  

Оценку качества построенной модели даёт индекс детерминации, а также средняя ошибка аппроксимации $\mathsf{ \overline A = \dfrac 1 n \sum |\dfrac{y - \overset{\wedge} y}{y}| \cdot 100\% }$

Допустимый предел значений $\mathsf{\overline A = \left(8-10\%\right)}$

  

Средний коэффициент эластичности

Показывает насколько % в среднем по совокупности изменится результат $\mathsf y$ от своей средней величины при изменения фактора $\mathsf x$ на 1% от своего среднего значения

$\mathsf{ \overline э = f'(\overline x) \cdot \dfrac{\overline x}{\overline y} }$ - для линейной регрессии

  

Задача дисперсионного анализа состоит в анализе дисперсии анализируемой переменной $\mathsf{ \sum(y - \overline y)^2 = \sum (\overset{\wedge} y_x - \overline y)^2 + \sum(y - \overset{\wedge} y_x)^2 }$, где до = - общая сумма квадратов отклонений, первое слагаемое - сумма квадратов отклонений, обусловленное регрессией, последнее - остаточная сумма квадратов отклонений.

  

Долю дисперсии, объясняемую регрессией, общей дисперсией результативного признака y характеризует коэффициент детерминации $\mathsf{ R^2 = \dfrac{\sum(\overset{\wedge} y_x - \overline y)^2}{\sum(y - \overline y)^2} }$ (индекс корреляции)

  

  

$\mathsf{ F_{тест} }$ для оценки качества уравнения регрессии состоит в проверке гипотезы $\mathsf{ H_0 }$ о статистической незначимости уравнения регрессии и показателя тесноты связи

$\mathsf{F}$ - критерия фишера

  

$\mathsf{ F_{факт} = \dfrac{\dfrac{\sum(\overset{\wedge} y - \overline y)^2}{m}}{\dfrac{\sum (y - \overset{\wedge} y)^2}{n - m - 1}} = \dfrac{r_{xy}^2}{1 - r_{xy}^2}(n - 2) }$,

где

$\mathsf n$ - число единиц совокупности

$\mathsf m$ - число параметров при переменных $\mathsf x$

$\mathsf{ F_{табл} }$ - максимально возможное значение критерия, под влиянием случайных факторов, при данных степенях свободы $\mathsf k$ и уровни значимости $\mathsf \alpha$

Уровень значимости $\alpha$ - вероятность отвергнуть $\mathsf{H_0}$, при условии, что она правильная

  

Если $\mathsf{ F_{табл} < F_{факт} \implies }$$\mathsf{ H_0 }$ - откл. и признаётся их статистическая значимость и надёжность

В противном случае (>)

  

  

Для оценки статистической значимость коэффициентов регрессии и корреляции рассчитывается $\mathsf t$ - Стьюдента и доверительный интервал каждого из показателей

Выдвигается гипотеза $\mathsf{ H_0 }$ - о случайной природе показателей (незначительное отличие от 0)

  

Оценка значимости коэффициентов регрессии и корреляции с помощью $\mathsf t$-Стьюдента проводится путём сопоставления их значений с величиной случайной ошибки

$\mathsf{ t_в = \dfrac{в}{m_в}; \quad t_a = \dfrac{a}{m_a}; \quad t_{r} = \dfrac{r}{m_r} }$

  

Случайные ошибки параметров линейной регрессии $\mathsf{m_в}$ и коэффициента корреляции определяются по формуле $\mathsf{ m_в = \sqrt{\dfrac{\dfrac{\sum(y - \overset{\wedge} y_x)^2}{n - 2}}{\sum(x - \overline x)^2}} = \sqrt{\dfrac{S_{ост}}{\sigma_x \sqrt n}} }$

$\mathsf{ m_а = \sqrt{\dfrac{\sum(y - \overset{\wedge} y_x)^2}{(n - 2)} \cdot \dfrac{\sum x^2}{n \cdot \sum (x - \overline x)^2}} = S_{ост}\dfrac{\sqrt{\sum x^2}}{n \sigma_x} }$

$\mathsf{ m_{r_{xy}} = \sqrt{\dfrac{1 - r_{xy}^2}{n - 2}} }$

  

Связь между $\mathsf t$-Стьюдента и $\mathsf{ F }$-Фишера

$\mathsf{ t_r^2 = t_в^2 = \sqrt F }$

  

Если $\mathsf{t_{таб} < t_{факт} \implies H_0}$ - отклоняется, т. е. $\mathsf{a, b, r_{xy}}$ - сформировались под действием критерия $\mathsf{x}$

  

Для расчёта доверительного интервала определяем предельную ошибку для каждого показателя.

$\mathsf{ \Delta_а = t_{табл} m_a }$

$\mathsf{ \Delta_в = t_{табл} m_в }$

  

$\mathsf{ \gamma_a = a \pm \Delta_a; \quad \gamma_{a\min} = a - \Delta_a; \quad \gamma_{a \max} = a + \Delta_a }$

То же самое для в

  

Если в границы доверительного интервала попадает 0, т. е. нижняя граница отрицательна, а верхняя положительная, то оценивая параметр, принимаем значение 0.

  

$\mathsf{ y_p }$ (прогнозное) определяется путём подстановки в уравнение регрессии $\mathsf{x_p}$.

  

Вычисляется среднее стандартное ошибка прогноза $\mathsf{ m_{\overset{\wedge} y p} = \sigma_{ост} \cdot \sqrt{1 + \dfrac 1 n + \dfrac{(x_p - \overline x)^2}{\sum(x - \overline x)^2}} }$, где $\mathsf{ \sigma_{ост} = \sqrt{\dfrac{\sum(y - \overset{\wedge} y)^2)}{n - m - 1}} }$

  

$\mathsf{ \sigma_{\overset{\wedge} y p} = \overset{\wedge} y_p \pm \Delta_{\overset{\wedge} y p} }$

$\mathsf{ \overset{\wedge} y_{\overset{\wedge} y p_{\min}} = \overset{\wedge} y_p - \Delta \overset{\wedge} y_p }$

$\mathsf{ \sigma_{\overset{\wedge} y p_{\max}} = \overset{\wedge} y_p + \Delta \overset{\wedge} y_p }$

$\mathsf{ \Delta \overset{\wedge} y_p = t_{табл} \cdot m_{\overset{\wedge} y p} }$