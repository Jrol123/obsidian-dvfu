# Тема 1. База
## Типы данных
## Понятие однородности / неоднородности данных
### Двойственность формулировки
## Методы предобработки
## Типы задач, решаемых ИИ
## Виды обучений
## Формальная постановка задач

# Тема 2. Регрессия
## Постановка регрессии как задачи оптимизации
## Метрики и функции ошибки задач регрессии
### Интерпретируемость и применимость метрик
## Вывод аналитического решения задачи линейной регрессии в векторной форме
при дифференцировании первое слагаемое можно сжато
$$
X_{n \times k} = \begin{pmatrix} 
x_{11} & x_{12} & \ldots &  x_{1k} \\
x_{21} & x_{22} & \ldots & {x_{2k}} \\ 
\vdots  & \vdots & \ddots & \vdots \\ 
x_{n1} & x_{k2} & \ldots & x_{n k}
\end{pmatrix}
$$
$$
\omega = \pmatrix{
\omega_{1} \\
\vdots \\
\omega_{k}
} \quad 
y = \pmatrix{
y_{1} \\
\vdots \\
y_{n}
}
$$
$$
\hat{y} = X \omega
$$
**Функция потерь**
$$
L = (X\omega - y)^{2} \to \min
$$

Преобразуем функцию потерь
$$
L = (X\omega-y)^{T} (X\omega-y) = (X\omega)^{T}X\omega - (X\omega)^{T} y - y^{T}X\omega + y^{T}y
$$

**Используя свойства векторной алгебры получаем тождества**
1. $y^{T}X\omega = (X\omega)^{T}y = \omega^{T}X^{T}y$
2. $(X\omega)^{T}X\omega = \omega^{T} X^{T} X \omega$

С учетом этих свойств, функция потерь принимает вид
$$
L = \omega^{T}X^{T}X\omega - 2\omega^{T}X^{T}y + y^{T}y
$$
Перейдем к решению задачи минимизации. Для этого продифференцируем функцию потерь по вектору весов
$$
\dfrac{dL}{d\omega} = \dfrac{d}{d\omega} \omega^{T}X^{T}X\omega -2 \dfrac{d}{d\omega}\omega^{T}X^{T}y + \dfrac{d}{d\omega}y^{T}y = L_{1} -2 L_{2} + L_{3}
$$
---
$$
w^{T}X^{T}Xw = w^{T} A w = (w_{1},\ldots, w_{k}) \pmatrix{
a_{11} && \ldots && a_{1k} \\
\ldots && \ldots && \ldots \\
a_{k1} && \ldots && a_{kk}
} \pmatrix{w_{1} \\ \ldots \\ w_{k}} =
$$
$$
= \pmatrix{
a_{11}w_{1} + \ldots + a_{k1} w_{k} ,~ \ldots ,~ a_{1k}w_{1} + \ldots + a_{kk} w_{k}
} \pmatrix{w_{1} \\ \ldots \\ w_{k}} =
$$
$$
= w_{1}(a_{11}w_{1} + \ldots + a_{k1} w_{k}) + \ldots + w_{k}(a_{1k}w_{1} + \ldots + a_{kk} w_{k}) = \sum\limits_{i=1}^{k} a_{ii} w^{2}_{i} + 2 \sum\limits_{i=1}^{k} \sum\limits_{j=1}^{i - 1} a_{ij} w_{i}
$$
$$
\pmatrix{
a_{11}w_{1}^{2} && a_{12}w_{1}w_{2} && \vdots && a_{1k}w_{1}w_{k} \\
a_{21}w_{2}w_{1} && a_{22}w_{2}^{2} && \vdots && a_{2k}w_{2}w_{k} \\
\ldots && \ldots && \ddots && \vdots \\
a_{k1}w_{k}w_{1} && a_{k2}w_{k}w_{2} && \ldots && a_{kk}w_{k}^{2}
}
$$
Получили квадратичную форму с матрицей $A$. 
С учетом этого получаем
$$
L_{1} = \pmatrix{
2\sum\limits_{i=1}^{k} a_{i1}w_{i} \\
2\sum\limits_{i=1}^{k} a_{i2}w_{i} \\
\vdots \\
2\sum\limits_{i=1}^{k} a_{ik}w_{i}
} = 2A\omega = 2X^{T}X\omega
$$
---
$$
w^{T}X^{T}y = (w_{1}, \ldots, w_{k}) \pmatrix{
x_{11}y_{1} + \ldots + x_{n1} y_{n} \\
\vdots \\
x_{1k}y_{1} + \ldots + x_{nk} y_{n}
} = 
$$
$$
= w_{1} (x_{11}y_{1} + \ldots + x_{n1}y_{n}) + \ldots + w_{k} (x_{1k}y_{1} + \ldots + x_{nk} y_{n})
$$
Таким образом
$$
L_{2} = \pmatrix{
x_{11}y_{1} + \ldots + x_{n1} y_{n} \\
\vdots \\
x_{1k}y_{1} + \ldots + x_{nk} y_{n}
} = X^{T}y
$$
---
$$
L_{3} = 0
$$
*Тривиально*

---
В конечном счете имеем условие экстремума
$$
\dfrac{dL}{d\omega} = 2X^{T}X \omega - 2X^{T} y = 0
$$
Находим оптимальный вектор весов
$$
2X^{T}X \omega - 2X^{T} y = 0
$$
$$
X^{T}X \omega = X^{T} y
$$
$$
\omega = (X^{T}X)^{-1} X^{T} y
$$

## Модели применяемые для решения задачи регрессии
## Внесение нелинейности в линейные модели
## Случаи использования

# Тема 3. Классификация
## Метрики (для бинарной и мультиклассификации)
## ROC AUC
## Гиперпараметр классификации (lr)
## Примеры выбора метрик для бинарной
## Модернизация метрик для задачи мультиклассификации. 
## Примеры выбора метрик для задачи мультиклассификации.
# Тема 4. Деревья
## Построение дерева для классификации (регрессии)
## Критерии останова
## Критерии разбиения (регрессия / классификация)
## Обработка категориальных признаков
# Тема 5. Кластеризация
# Тема 6. Оптимизаторы
# Тема 7. Кроссвалидация
# Тема 8. Airflow
# Тема 9. Mlflow
# Тема 10. Рекомендательные системы


