# Тема 1. База
## Типы данных
## Понятие однородности / неоднородности данных
### Двойственность формулировки
## Методы предобработки
## Типы задач, решаемых ИИ
## Виды обучений
## Формальная постановка задач

# Тема 2. Регрессия
## Постановка регрессии как задачи оптимизации
## Метрики и функции ошибки задач регрессии
### Интерпретируемость и применимость метрик
## Вывод аналитического решения задачи линейной регрессии в векторной форме
при дифференцировании первое слагаемое можно сжато


$X_{n+1 \times k+1} = \begin{pmatrix}x_{00} & x_{01} & \ldots &  x_{0k} \\ x_{10} & \ldots & \ldots & \ldots \\ \ldots  & \ldots & \ldots & \ldots \\ x_{n0} & x_{k1} & \ldots & x_{n k}\end{pmatrix}$
$A = X^{T}X$
$y \in R$

$f\left( w, x \right) = X\overline{ w }$
$w = \begin{pmatrix}w_{0} \\ w_{1} \\ \ldots \\ w_{k} \end{pmatrix} \quad y = \begin{pmatrix}y_{0} \\ y_{1} \\ \ldots \\ y_{n}\end{pmatrix}$

Лосс-функция
	$L = \left( X\overline{ w } - \overline{ y } \right)^{2} \to \min$
Преобразование
	$\left( X\overline{ w } - \overline{ y } \right)^{2} = \left( X\overline{ w } - \overline{ y } \right)^{T}\left( X\overline{ w } - \overline{ y } \right) = \left( X\overline{ w } \right)^{T}X\overline{ w } - \overline{ y }^{T}X\overline{ w } - \left( X\overline{ w } \right)^{T}\overline{ y } + \overline{ y }^{T}\overline{ y }$

$$
1.1. y^TXw=(Xw)^Ty = w^TX^T y
$$
$$
(Xw)^T Xw=w^TX^T Xw
$$
$$
1.2. L= w^T X^TXw - 2w^TX^T y + y^T y
$$
$$
2. \dfrac{dL}{dw} = 2X^T X w - 2 X^T y = 0 \implies X^T X w = X^T y
$$
### Дифференцирование
$$
w^{T}X^{T}Xw = w^{T} A w = (w_{0},\ldots, w_{k}) \pmatrix{
a_{00} && \ldots && a_{0k} \\
\ldots && \ldots && \ldots \\
a_{k0} && \ldots && a_{kk}
} \pmatrix{w_{0} \\ \ldots \\ w_{k}} =
$$
$$
= \pmatrix{
a_{00}w_{0} + \ldots + a_{k0} w_{k} ,~ \ldots ,~ a_{0k}w_{0} + \ldots + a_{kk} w_{k}
} \pmatrix{w_{0} \\ \ldots \\ w_{k}} =
$$
$$
= w_{0}(a_{00}w_{0} + \ldots + a_{k0} w_{k}) + \ldots + w_{k}(a_{0k}w_{0} + \ldots + a_{kk} w_{k}) = \sum\limits_{i=0}^{k} a_{ii} w^{2}_{i} + 2 \sum\limits_{i=0}^{k} \sum\limits_{j=0}^{i - 1} a_{ij} w_{i}
$$
$$
\dfrac{d(w^{T}X^{T}Xw)}{dw} = 2X^{T} X w
$$

$$
w^{T}X^{T}y = (w_{0}, \ldots, w_{k}) \pmatrix{
x_{00}y_{0} + \ldots + x_{n0} y_{n} \\
\ldots \\
x_{0k}y_{0} + \ldots + x_{nk} y_{n}
} = 
$$
$$
= w_{0} (x_{00}y_{0} + \ldots + x_{n0}y_{n}) + \ldots + w_{k} (x_{0k}y_{0} + \ldots + x_{nk} y_{n})
$$

$$
\dfrac{d(w^{T}X^{T}y)}{dw} = \dfrac{d}{dw}\left( w_{0} (x_{00}y_{0} + \ldots + x_{n0}y_{n}) + \ldots + w_{k} (x_{0k}y_{0} + \ldots + x_{nk} y_{n}) \right) = X^{T} y
$$

## Модели применяемые для решения задачи регрессии
## Внесение нелинейности в линейные модели
## Случаи использования

# Тема 3. Классификация
## Метрики (для бинарной и мультиклассификации)
## ROC AUC
## Гиперпараметр классификации (lr)
## Примеры выбора метрик для бинарной
## Модернизация метрик для задачи мультиклассификации. 
## Примеры выбора метрик для задачи мультиклассификации.
# Тема 4. Деревья
## Построение дерева для классификации (регрессии)
## Критерии останова
## Критерии разбиения (регрессия / классификация)
## Обработка категориальных признаков
# Тема 5. Кластеризация
# Тема 6. Оптимизаторы
# Тема 7. Кроссвалидация
# Тема 8. Airflow
# Тема 9. Mlflow
# Тема 10. Рекомендательные системы


