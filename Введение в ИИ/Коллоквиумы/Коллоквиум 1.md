# Тема 1. База
## Типы данных
```mermaid
%%{init: {'themeVariables': { 'fontSize': '14px', 'nodeBorder': '#000', 'clusterBkg': 'white' }}}%%
graph TB
    A00(["Данные"]) --> A10(["Количественные"])
    A00 --> A11(["Качественные (Категориальные)"])
    
    A10 --> A20(["Дискретные"])
  A10 --> A21(["Непрерывные"])
  
  A11 --> A22(["Порядковые"])
  A11 --> A23(["Номинальные"])

```
### Дискретные 
Целые значения.
Обычно результат счёта.
### Непрерывные
float / диапазон.
Обычно результат измерения

### Порядковые
Их нельзя складывать
S < M < L < XL < XXL
(Можно упорядочить, как, например, размер одежды)
(Label encoding)
### Номинальные
Их нельзя складывать и сравнивать
Бензин $\cancel{ < }$ Дизель
(OneHot encoding)
## Понятие однородности / неоднородности данных
### По нормальному
#### Однородный
Данные однородные если они имеют одинаковую природу.
Сигнал, звук, изображение, изменение по времени какой-либо величины и т.д.
#### Неоднородный
Если в наборе есть и числовые и категориальные признаки / из разных источников / разный масштаб

### По ML
#### Однородный
- Каждая строка — независимый объект
- Порядок строк не важен (перемешивание не меняет свойства данных)
- Нет временных или пространственных зависимостей
- Все данные из одного распределения
#### Неоднородный
- Временные зависимости (порядок дней)
- Пространственные зависимости (изображение)
- Групповые зависимости (измерения внутри одного пациента зависимы)
- Разные распределение (данные собраны в разных условиях)
#### Как бороться с неоднородностью?
- **Для временных рядов:** Использовать специальные методы валидации (временные разбиения)
- **Для пространственных данных:** Учитывать пространственные autocorrelation
- **Для групповых данных:** Использовать групповую валидацию (GroupKFold)
- **Для разных распределений:** Техники domain adaptation (Перенос знаний с модели на модель)

## Методы предобработки
### Missing data
Пропущенные значения
- **Удаление**
	- Потеря информации
- **Заполнение**
	- **Числовые:** Среднее, медиана, мода
	- **Категориальные:** мода или категория *Unknown*
### Encoding
Перевод категориальных данных в количественные
- **Label Ecoding**
	- М, Ж = 0, 1
		- Кодируем каждый признак целым числом
	- Модель может решить, что между числами есть порядок
- **One-Hot encoding**
	- М, Ж = [1, 0], [0, 1]
		- Каждая координата отвечает за конкретное значение. В данном случае первый столбец - $is\_М$, второй - $is\_Ж$
	- *проклятие размерности*
		- При большом количестве категорий матрица становится разреженной и большой
### Масштабирование и нормализация
Уменьшение масштаба данных.
- **Стандартизирование**
	- $\dfrac{x - \mu}{\sigma}$
	- Сведение к среднему = 0 и стандартному отклонению = 1
- **Min-max**
	- $\dfrac{x - \min}{\max - \min}$
	- Приведение значений к диапазону [0, 1]
	- *Сохраняет исходное распределение*
### Преобразование признаков
Создание новых признаков из существующих для лучшего описания закономерностей.
- **Полиномиальные признаки**
	- Для учёта нелинейных зависимостей
	- $x^{2},\ x^{3},\ x_{1}\cdot x_{2}$
- **Дискретизация**
	- Перевод непрерывного признака в категориальный / интервальный
	- возраст → [0-18, 19-65, 66+]
- **Извлечение признаков**
	- **Из дат:** день недели, месяц, является ли выходным 
	- **Из текста:** длина, наличие ключевых слов
### Работа с выбросами
Убирание значений, сильно отклоняющихся от остальных наблюдений
- **Ящик с усами**
	- Берём 25 и 75 перцентиль; отнимаем / прибавляем $1.5 \cdot \operatorname{IQR}$ (интерквартильный размах) (a. k. a. делаем усы); Смотрим, что будет вне границ
		- $x< Q_{1} - 1.5 \cdot \operatorname{IQR}$
		- $x> Q_{3} + 1.5 \cdot \operatorname{IQR}$
Можем удалить, заменить на предельное значение или же как-то их преобразовать.
## Типы задач, решаемых ИИ
### Задачи с учителем
Если есть размеченные данные, пытаемся предсказать правильный ответ
**Задачи:**
- Регрессия
- Классификация
	- Бинарная, многоклассовая
- Сегментация
	- Разделение изображения на смысловые области
- Ранжирование
	- Упорядочивание объектов
### Без учителя
Нет размеченных данных, ищем скрытые структуры
**Задачи:**
- Кластеризация
	- Группировка похожих объектов
- Понижение размерности
	- Сокращение числа признаков
- Детекция аномалий
	- Поиск выбросов
- Генерация
	- Создание новых данных (GAN)
### С частичным контролем
Мало размеченных данных + много неразмеченных
- Обычно небольшое количество размеченных данных и большое количество неразмеченных.
- Неразмеченные данные помогают понять структуру пространства, а размеченные - определить границы классов
- Неразмеченные данные дают представление о структуре классифицируемого множества объектов, тогда как размеченные данные определяют классификацию в пределах этой структуры.
### Обучение с подкреплением
Агент учится взаимодействовать со средой и получает "награды" за правильные действия
## Формальная постановка задач
1. **Какие данные есть / могут быть получены?**
2. **Какой тип величины мы можем прогнозировать на основе данных?**
	1. Определение вида задачи
3. **Какой характер данных и зависимостей в них?**
	1. Структура данных, зависимости и их характер
4. **Какие у нас есть ресурсы (технические и человекоресурсы)?**
	1. Время обучения, память, GPU
	2. Эксперты для разметки данных и компетенции команды в ML
	3. Требования к точности и интерпретируемости
	4. Стоимость ошибки (FP, FN)

### Пример формальной постановки
**Задача**: Прогноз оттока клиентов банка
1. **Данные**:
    - 100K клиентов, 50 признаков (возраст, баланс, количество операций)
    - Есть пропуски в данных о доходе
2. **Целевая переменная**:
    - Бинарная: ушел/не ушел (классификация)
3. **Характер данных**:
    - Табличные данные, временные ряды операций
    - Наблюдения независимы (i.i.d.)
    - Признаки: числовые + категориальные
4. **Ресурсы**:
    - Сервер с 16ГБ RAM
    - Модель должна давать ответ < 1 секунды
    - Важна интерпретируемость (чтобы понимать причины оттока)

# Тема 2. Регрессия
## Постановка регрессии как задачи оптимизации
## Метрики и функции ошибки задач регрессии
### Интерпретируемость и применимость метрик
## Вывод аналитического решения задачи линейной регрессии в векторной форме
при дифференцировании первое слагаемое можно сжато
$$
X_{n \times k} = \begin{pmatrix} 
x_{11} & x_{12} & \ldots &  x_{1k} \\
x_{21} & x_{22} & \ldots & {x_{2k}} \\ 
\vdots  & \vdots & \ddots & \vdots \\ 
x_{n1} & x_{k2} & \ldots & x_{n k}
\end{pmatrix}
$$
$$
\omega = \pmatrix{
\omega_{1} \\
\vdots \\
\omega_{k}
} \quad 
y = \pmatrix{
y_{1} \\
\vdots \\
y_{n}
}
$$
$$
\hat{y} = X \omega
$$
**Функция потерь**
$$
L = (X\omega - y)^{2} \to \min
$$

Преобразуем функцию потерь
$$
L = (X\omega-y)^{T} (X\omega-y) = (X\omega)^{T}X\omega - (X\omega)^{T} y - y^{T}X\omega + y^{T}y
$$

**Используя свойства векторной алгебры получаем тождества**
1. $y^{T}X\omega = (X\omega)^{T}y = \omega^{T}X^{T}y$
2. $(X\omega)^{T}X\omega = \omega^{T} X^{T} X \omega$

С учетом этих свойств, функция потерь принимает вид
$$
L = \omega^{T}X^{T}X\omega - 2\omega^{T}X^{T}y + y^{T}y
$$
Перейдем к решению задачи минимизации. Для этого продифференцируем функцию потерь по вектору весов
$$
\dfrac{dL}{d\omega} = \dfrac{d}{d\omega} \omega^{T}X^{T}X\omega -2 \dfrac{d}{d\omega}\omega^{T}X^{T}y + \dfrac{d}{d\omega}y^{T}y = L_{1} -2 L_{2} + L_{3}
$$
---
$$
w^{T}X^{T}Xw = w^{T} A w = (w_{1},\ldots, w_{k}) \pmatrix{
a_{11} && \ldots && a_{1k} \\
\ldots && \ldots && \ldots \\
a_{k1} && \ldots && a_{kk}
} \pmatrix{w_{1} \\ \ldots \\ w_{k}} =
$$
$$
= \pmatrix{
a_{11}w_{1} + \ldots + a_{k1} w_{k} ,~ \ldots ,~ a_{1k}w_{1} + \ldots + a_{kk} w_{k}
} \pmatrix{w_{1} \\ \ldots \\ w_{k}} =
$$
$$
= w_{1}(a_{11}w_{1} + \ldots + a_{k1} w_{k}) + \ldots + w_{k}(a_{1k}w_{1} + \ldots + a_{kk} w_{k}) = \sum\limits_{i=1}^{k} a_{ii} w^{2}_{i} + 2 \sum\limits_{i=1}^{k} \sum\limits_{j=1}^{i - 1} a_{ij} w_{i}
$$
$$
\dfrac{d}{w_{2}}\pmatrix{
a_{11}w_{1}^{2} && a_{12}w_{1}w_{2} && \vdots && a_{1k}w_{1}w_{k} \\
a_{21}w_{2}w_{1} && a_{22}w_{2}^{2} && \vdots && a_{2k}w_{2}w_{k} \\
\ldots && \ldots && \ddots && \vdots \\
a_{k1}w_{k}w_{1} && a_{k2}w_{k}w_{2} && \ldots && a_{kk}w_{k}^{2}
} = 2\sum \limits_{ i = 1 }^{ k  } a_{2i}\cdot w_{i} 
$$
Получили квадратичную форму с матрицей $A$. 
С учетом этого получаем
$$
L_{1} = \pmatrix{
2\sum\limits_{i=1}^{k} a_{i1}w_{i} \\
2\sum\limits_{i=1}^{k} a_{i2}w_{i} \\
\vdots \\
2\sum\limits_{i=1}^{k} a_{ik}w_{i}
} = 2A\omega = 2X^{T}X\omega
$$
---
$$
w^{T}X^{T}y = (w_{1}, \ldots, w_{k}) \pmatrix{
x_{11}y_{1} + \ldots + x_{n1} y_{n} \\
\vdots \\
x_{1k}y_{1} + \ldots + x_{nk} y_{n}
} = 
$$
$$
= w_{1} (x_{11}y_{1} + \ldots + x_{n1}y_{n}) + \ldots + w_{k} (x_{1k}y_{1} + \ldots + x_{nk} y_{n})
$$
Таким образом
$$
L_{2} = \pmatrix{
x_{11}y_{1} + \ldots + x_{n1} y_{n} \\
\vdots \\
x_{1k}y_{1} + \ldots + x_{nk} y_{n}
} = X^{T}y
$$
---
$$
L_{3} = 0
$$
*Тривиально*

---
В конечном счете имеем условие экстремума
$$
\dfrac{dL}{d\omega} = 2X^{T}X \omega - 2X^{T} y = 0
$$
Находим оптимальный вектор весов
$$
2X^{T}X \omega - 2X^{T} y = 0
$$
$$
X^{T}X \omega = X^{T} y
$$
$$
\omega = (X^{T}X)^{-1} X^{T} y
$$

## Модели применяемые для решения задачи регрессии
## Внесение нелинейности в линейные модели
## Случаи использования

# Тема 3. Классификация
## Метрики (для бинарной и мультиклассификации)
## ROC AUC
## Гиперпараметр классификации (lr)
## Примеры выбора метрик для бинарной
## Модернизация метрик для задачи мультиклассификации. 
## Примеры выбора метрик для задачи мультиклассификации.
# Тема 4. Деревья
## Построение дерева для классификации (регрессии)
## Критерии останова
## Критерии разбиения (регрессия / классификация)
## Обработка категориальных признаков
# Тема 5. Кластеризация
https://education.yandex.ru/handbook/ml/article/klasterizaciya
## Постановка задачи
Задача **обучения без учителя**, целью которой является разбиение множества объектов на группы (кластеры) таким образом, чтобы:
- Объекты внутри одного кластера были **максимально похожи** друг на друга
- Объекты из разных кластеров были **максимально различны**
$C_{i}$ -кластер. Кластеры не пересекаются
$\mu_i$ — центр кластера $C_i$
$d\left( x_{i}, x_{j} \right)$ - мера близости между объектами.

**Критерий качества:**
- Минимизация внутрикластерного расстояния:  
    $\min \sum \limits_{i=1}^k \sum_{x \in C_i} d(x, \mu_i)$
- Максимизация межкластерного расстояния:  
    $\max \sum_{i \neq j} d(\mu_i, \mu_j)$  

**Число кластеров:**
- Может быть задано априори ($k$-means)
- Может определяться автоматически (DBSCAN, иерархическая кластеризация)
## K-means
### Идея
Разбить данные на **k кластеров** так, чтобы минимизировать внутрикластерную вариацию (сумму квадратов расстояний от точек до центроида их кластера).

### Алгоритм
**Инициализация:**
- Выбираем число кластеров **k**
- Случайно инициализируем **k центроидов** (центров кластеров)
**Основной цикл:**
1. **Назначение кластеров (E-step):**
    - Для каждой точки находим ближайший центроид
    - Назначаем точку соответствующему кластеру
    - _Формально:_ $C_i = {x_j : ||x_j - \mu_i||^2 \leq ||x_j - \mu_l||^2 \ \forall l}$
2. **Пересчет центроидов (M-step):**
    
    - Для каждого кластера вычисляем новый центроид как среднее всех точек кластера
        
    - _Формально:_ $\mu_i = \frac{1}{|C_i|} \sum_{x_j \in C_i} x_j$
        
3. **Проверка сходимости:**
    
    - Если центроиды не изменились (или изменения меньше порога) → **СТОП**
        
    - Иначе → повторяем шаги 1-2
## Улучшения по сходимости / времени
## DBSCAN
## Алгоритм нахождения кластеров
по ближайшей центроиде
## Метрики кластеризации

## Иерархические и агломерационные методы кластеризации



