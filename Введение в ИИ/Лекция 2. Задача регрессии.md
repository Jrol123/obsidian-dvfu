Хавайте: https://habr.com/ru/articles/479398/

$X_{n \times k} = \begin{pmatrix}x_{00} & x_{01} & \ldots &  x_{0k} \\ x_{10} & \ldots & \ldots & \ldots \\ \ldots  & \ldots & \ldots & \ldots \\ x_{n0} & x_{k1} & \ldots & x_{n k}\end{pmatrix}$
$A = X^{T}X$
$y \in R$

$f\left( w, x \right) = X\overline{ w }$
$w = \begin{pmatrix}w_{0} \\ w_{1} \\ \ldots \\ w_{k} \end{pmatrix} \quad y = \begin{pmatrix}y_{0} \\ y_{1} \\ \ldots \\ y_{n}\end{pmatrix}$

Лосс-функция
	$L = \left( X\overline{ w } - \overline{ y } \right) \to \min$
Преобразование
	$\left( X\overline{ w } - \overline{ y } \right)^{2} = \left( X\overline{ w } - \overline{ y } \right)^{T}\left( X\overline{ w } - \overline{ y } \right) = \left( X\overline{ w } \right)^{T}X\overline{ w } - \overline{ y }^{T}X\overline{ w } - \left( X\overline{ w } \right)^{T}\overline{ y } + \overline{ y }^{T}\overline{ y }$

$$
1.1. y^TXw=(Xw)^Ty = w^TX^T y
$$
$$
(Xw)^T Xw=w^TX^T Xw
$$
$$
1.2. L= w^T X^TXw - 2w^TX^T y + y^T y
$$
$$
2. \dfrac{dL}{dw} = 2X^T X w - 2 X^T y = 0 \implies X^T X w = X^T y
$$
### Дифференцирование
$$
w^{T}X^{T}Xw = w^{T} A w = (w_{0},\ldots, w_{k}) \pmatrix{
a_{00} && \ldots && a_{0k} \\
\ldots && \ldots && \ldots \\
a_{k0} && \ldots && a_{kk}
} \pmatrix{w_{0} \\ \ldots \\ w_{k}} =
$$
$$
= \pmatrix{
a_{00}w_{0} + \ldots + a_{k0} w_{k} ,~ \ldots ,~ a_{0k}w_{0} + \ldots + a_{kk} w_{k}
} \pmatrix{w_{0} \\ \ldots \\ w_{k}} =
$$
$$
= w_{0}(a_{00}w_{0} + \ldots + a_{k0} w_{k}) + \ldots + w_{k}(a_{0k}w_{0} + \ldots + a_{kk} w_{k}) = \sum\limits_{i=0}^{k} a_{ii} w^{2}_{i} + 2 \sum\limits_{i=0}^{k} \sum\limits_{j=0}^{i - 1} a_{ij} w_{i}
$$
$$
\dfrac{d(w^{T}X^{T}Xw)}{dw} = 2X^{T} X w
$$

$$
w^{T}X^{T}y = (w_{0}, \ldots, w_{k}) \pmatrix{
x_{00}y_{0} + \ldots + x_{n0} y_{n} \\
\ldots \\
x_{0k}y_{0} + \ldots + x_{nk} y_{n}
} = 
$$
$$
= w_{0} (x_{00}y_{0} + \ldots + x_{n0}y_{n}) + \ldots + w_{k} (x_{0k}y_{0} + \ldots + x_{nk} y_{n})
$$

$$
\dfrac{d(w^{T}X^{T}y)}{dw} = \dfrac{d}{dw}\left( w_{0} (x_{00}y_{0} + \ldots + x_{n0}y_{n}) + \ldots + w_{k} (x_{0k}y_{0} + \ldots + x_{nk} y_{n}) \right) = X^{T} y
$$



# Методы ошибки
## MAE
$$
MAE = \dfrac 1 n \sum\limits_1^n |\hat y - y|
$$
## MSE
$$
\dfrac 1 n \sum \left( \hat{y} - y \right)^{2}  
$$
## MAPE
$$
MAPE = \dfrac 1 n \sum \dfrac{|y-\hat y|}{|y|}
$$
## $R^{2}$
$$
R^2 = 1 - \dfrac{\sum (\hat y - y)^2}{\sum (\hat y - \overline y}
$$
## RMSE
$$
RMSE = \sqrt{ \dfrac 1 n \sum \limits_{ 1 }^{ n } \left( \hat{y} - y \right) ^{2} }
$$







