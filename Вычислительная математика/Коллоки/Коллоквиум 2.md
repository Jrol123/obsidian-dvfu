# 1. Проблема собственных значений. Устойчивость
$B = T^{-1}AT$
$\lambda(A) = \lambda(B)$
$A$ и $B$ подобны
## Проблема собственных значений
**Полная проблема**
	Нахождение всех собственных значений и собственных векторов некоторой матрицы
**Частичная проблема собственных значений**
	Нахождение нескольких собственных значений и соответствующих им векторов
## Устойчивость
Устойчивость это то, насколько сильно меняются собственные значения и собственные векторы при небольших изменениях в исходной матрице.
A. k. a. [[Коллоквиум 0#4. Число обусловленности матрицы. Определение. Вычислительн формула. Пример плохо обусловленной системы.|Число обусловленности]]

# 2. Вариационное описание собственных значений симметрической матрицы
## Условия
Пусть $\lambda_{i}(A) \gt 0$ и $\lambda_{1}(A) \leq \lambda_{2}(A) \leq \ldots \leq \lambda_{n}(A)$
Пусть $y = U^{T}x$
$U$ - ортогональная матрица, где $u^{(i)}$ - собственный вектор
$D$ - диагональная матрица, где $d^{(i)} = \lambda_{i}(A)$
$\dfrac{\left( Ax, x \right)}{(x, x)} = \dfrac{\left( UDU^{T}x, x \right)}{(x, x)} = \dfrac{\left( DU^{T}x, U^{T}x \right)}{\left( U^{T}x, U^{T}x \right)} = \dfrac{\left( Dy, y \right)}{\left( y, y \right)}$
В силу невырожденности $U$: 
$\underset{ x \neq 0 }{ \min } \dfrac{\left( Ax, x \right)}{\left( x, x \right)} = \underset{ y \neq 0 }{ \min } \dfrac{\left( Dy, y \right)}{\left( y, y \right)}$

## Вариационное описание
**Отношение Рэлея**
	$R(A, x) = \dfrac{\left( Ax, x \right)}{(x, x)} = \dfrac{x^{T}Ax}{x^{T}x}$
**Вариационное описание**
	$\lambda_{1}(A) \leq \lambda_{2}(A) \leq \ldots \leq \lambda_{n}(A)$
	$\underset{ \begin{matrix}x \neq 0 \\ \left( x, u^{(1)} \right) = 0 \\ \ldots \\ \left( x, u^{(i - 1)} \right) = 0\end{matrix} }{ \min } \dfrac{\left( Ax, x \right)}{\left( x, x \right)} = \lambda_{i}(A) \quad i = 1, 2, \ldots, n$

## Доказательство

Покажем, что $\underset{ y \neq 0 }{ \min } \dfrac{\left( Dy, y \right)}{\left( y, y \right)} = \lambda_{1}(A)$, где $\lambda_{1} - \min$
$\dfrac{\left( Dy, y \right)}{\left( y, y \right)} = \dfrac{\sum \limits_{ i=1 }^{ n }\lambda_{i}\left( A \right)y_{i}^{2}}{\sum \limits_{ i=1 }^{ n }y_{i}^{2}} \geq \dfrac{\sum \limits_{ i=1 }^{ n }\lambda_{1}\left( A \right)y_{i}^{2}}{\sum \limits_{ i=1 }^{ n }y_{i}^{2}} = \lambda_{1}(A)$
Тогда
$y = e_{1} = \begin{pmatrix}1 \\ 0 \\ \vdots \\ 0\end{pmatrix}$
$\dfrac{\left( De_{1}, e_{1} \right)}{\left( e_{1}, e_{1} \right)} = \dfrac{\lambda_{1}(A)}{1}$
Очевидно, что минимальное значение отношения Рэлея достигается при $x = Ue_{1}$

Добавим условие $\left( x, u^{(1)} \right) = 0$
$\underset{ \begin{matrix}x \neq 0 \\ \left( x, u^{(1)} \right) = 0\end{matrix}}{ \min } \dfrac{\left( Ax, x \right)}{\left( x, x \right)} = \underset{ \begin{matrix}y \neq 0 \\ \left( y, e_{1} \right) = 0\end{matrix} }{ \min } \dfrac{\left( Dy, y \right)}{\left( y, y \right)}$
Получим, что $y_{1} = 0$. Поэтому для всех $y$, где $y \neq 0$ и $\left( y, e_{1} \right) = 0$
$\dfrac{\left( Dy, y \right)}{\left( y, y \right)} = \dfrac{\sum \limits_{ i = 2 }^{ k }\lambda_i(A)y_{i}^{2}}{\sum \limits_{ i = 2}^{ k }y_{i}^{2}} \geq \lambda_{2}(A)$
И если взять $y = e_{2} = \begin{pmatrix}0 \\ 1 \\ \vdots \\ 0\end{pmatrix}$, то получим:
$\underset{ \begin{matrix}x \neq 0 \\ \left( x, u^{(1)} \right) = 0\end{matrix}}{ \min } \dfrac{\left( Ax, x \right)}{\left( x, x \right)} = \underset{ \begin{matrix}y \neq 0 \\ \left( y, e_{1} \right) = 0\end{matrix} }{ \min } \dfrac{\left( Dy, y \right)}{\left( y, y \right)} =\lambda_{2}$

# 3. Вариационный принцип Куранта-Фишера
## Вариационный принцип Куранта-Фишера
$\underset{ R_{n - i + 1} }{ \max }\underset{ \begin{matrix}x \neq 0 \\ x \in R_{n - i + 1}\end{matrix} }{ \min } \dfrac{\left( Ax, x \right)}{\left( x, x \right)} = \lambda_{i}(A) \quad i = 1, 2, \ldots, n$
## Доказательство
Покажем, что если $S_{i}$ - подпространство, порождённое собственными векторами $u^{(1)}, \ldots, u^{(i)}$, то для всех $x \neq 0, x \in S_{i}$:
$\dfrac{\left( Ax, x \right)}{\left( x, x \right)} \leq \lambda_{i}(A)$

Т. к. $x \in S_{i}$, то можно представить $x = \sum \limits_{ j = 1 }^{ i }c_{j}u^{(j)}$

$\left( x, x \right) = \sum \limits_{ j = 1 }^{ i }c_{j}^{2} \quad \left( Ax, x \right) = \sum \limits_{ j = 1 }^{ i }\lambda_{j}(A)c_{j}^{2}$
$\dfrac{\left( Ax, x \right)}{\left( x, x \right)} = \dfrac{\sum \limits_{ j = 1 }^{ i }\lambda_{j}\left( A \right)c_{j}^{2}}{\sum \limits_{ j = 1 }^{ i }c_{j}^{2}} \leq \lambda_{i}(A)$

Пусть $R_{n - i + 1}$ - подпространство размерности $n - i + 1$.
Сумма размерностей $R_{n - i + 1}$ и $S_{i}$ равна $n + 1$, т. е. превышает размерность $n$ рассматриваемого пространства, т. е. существует нетривиальное пересечение.
Следовательно $\exists x_{0} \neq 0 \quad x \in R_{n - i + 1} \cap S_{i}$
Следовательно, $\dfrac{\left( Ax_{0}, x_{0} \right)}{\left( x_{0}, x_{0} \right)}\leq \lambda_{i}\left( A \right)$

Поскольку $x_{0} \in R_{n - i + 1}$, то $\underset{ \begin{matrix}x \neq 0 \\ x \in R_{n - i + 1}\end{matrix} }{ \min } \dfrac{\left( Ax, x \right)}{\left( x, x \right)} \leq \lambda_{i}\left( A \right)$

Соответственно, для того, чтобы наше неравенство стало равенством, нужно, чтобы наше подпространство состояло из векторов, ортогональных собственным векторам $u^{(1)}, \ldots, u^{(i - 1)}$.
Итого:
$\underset{ R_{n - i + 1} }{ \max }\underset{ \begin{matrix}x \neq 0 \\ x \in R_{n - i + 1}\end{matrix} }{ \min } \dfrac{\left( Ax, x \right)}{\left( x, x \right)} = \lambda_{i}(A)$

# 4. Два следствия из вариационного принципа Куранта-Фишера
## Следствие 1
Пусть $A = A^{T},\ B = B^{T}$
$\lambda_{1}(A) \leq \lambda_{2}(A) \leq \ldots \leq \lambda_{n}(A)$
$\lambda_{1}(B) \leq \lambda_{2}(B) \leq \ldots \leq \lambda_{n}(B)$
И пусть для всех $x$ выполняется:
$\left( Ax, x \right) \leq \left( Bx, x \right)$

Тогда:
$\lambda_{i}(A) \leq \lambda_{i}(B)$
## Доказательство следствия 1
Для всех $x \neq 0$ выполняется: $\dfrac{\left( Ax, x \right)}{\left( x, x \right)} \leq \dfrac{\left( Bx, x \right)}{\left( x, x \right)}$
Следовательно, для любого $R_{n - i+ 1}$:
$\underset{ \begin{matrix}x \neq 0 \\ x \in R_{n - i + 1}\end{matrix} }{ \min } \dfrac{\left( Ax, x \right)}{\left( x, x \right)} \leq \underset{ \begin{matrix}x \neq 0 \\ x \in R_{n - i + 1}\end{matrix} }{ \min } \dfrac{\left( Bx, x \right)}{\left( x, x \right)}$
Очевидно, что:
$\underset{ R_{n - i + 1} }{ \max }\underset{ \begin{matrix}x \neq 0 \\ x \in R_{n - i + 1}\end{matrix} }{ \min } \dfrac{\left( Ax, x \right)}{\left( x, x \right)} \leq \underset{ R_{n - i + 1} }{ \max }\underset{ \begin{matrix}x \neq 0 \\ x \in R_{n - i + 1}\end{matrix} }{ \min } \dfrac{\left( Bx, x \right)}{\left( x, x \right)}$
И согласно принципу Куранта-Фишера, это может быть записано в виде:
$\lambda_{i}(A) \leq \lambda_{i}(B)$
## Следствие 2
Пусть $A = A^{T},\ B = B^{T}$
$\lambda_{1}(A) \leq \lambda_{2}(A) \leq \ldots \leq \lambda_{n}(A)$
$\lambda_{1}(B) \leq \lambda_{2}(B) \leq \ldots \leq \lambda_{n}(B)$

Тогда:
$\left| \lambda_{i}(B) - \lambda_{i}(A) \right| \leq ||B - A||_{2}$

## Доказательство следствия 2
$C = B - A$
$\left( Bx, x \right) = \left( \left( A + C \right)x, x \right) = \left( Ax, x \right) + \left( Cx, x \right)$
Оценим $\left| \left( Cx, x \right) \right|$
$\left| \left( Cx, x \right) \right| \leq ||Cx||_{2} \cdot ||x||_{2} \leq ||C||_{2} \cdot ||x||_{2}^{2} = \left( ||C||_{2}Ex, x \right)$
Отсюда:
$\left( -||C||_{2}Ex, x \right)\leq \left( Cx, x \right)\leq \left( ||C||_{2}Ex, x \right)$
Следовательно,
$\left( \left( A - ||C||_{2}E \right)x, x \right) \leq \left( Bx, x \right) \leq \left( \left( A + ||C||_{2}E \right)x, x \right)$
Согласно [[#Следствие 1|следствию 1]]:
$\lambda_{i}\left( A - ||C||_{2}E \right)\leq\lambda_{i}\left( B \right)\leq\lambda_{i}\left( A + ||C||_{2}E \right)$
Или
$\lambda_{i}\left( A \right) - ||C||_{2}\leq\lambda_{i}(B) \leq \lambda_{i}\left( A \right) + ||C||_{2}$

# 5. Метод вращения решения проблем собственных значений. Типичный шаг метода
## Условия
Для всякой симметрической матрицы $A$ существует такая ортогональная матрица $U$, что 
$U^T A U = \Lambda$, где $\Lambda$ - диагональная матрица.
При этом диагональные элементы матрицы $\Lambda$ являются собственными значениями, а столбцы матрицы $U$ - собственными векторами матрицы $A$. 
Метод вращений состоит в построении таких ортогональных матриц $U_k$, что последовательность $A_{k+1} = U_k^T A U_k$ при $k \to \infty$ стремится к диагональной матрице $\Lambda$. 
Каждая $U_k$ в этом методе представляет собой произведение $k$ матриц вращений, имеющих вид:
$$ C_{lm} = \begin{pmatrix}1 &0&&&&&&& 0 \\ 0& \dots &&&&&&0 \\ && c && \ldots  & & -s\\ && \vdots & 1 &&&\vdots \\ &&&& \ldots && \\ &&\vdots &&& 1 &\vdots \\ &&s &&\ldots && c \\ & 0 & &&&&&\ldots & 0 \\ 0 &&&&&&& 0 & 1 \end{pmatrix} \begin{matrix}\\- \  l\text{-ая строка} \\\\\\\\\\ - \ m\text{-ая строка} \\\\  \end{matrix}$$
Параметры $c, s$, определяющие матрицу $C_{ij}$, называются опорными элементами и удовлетворяют равенству $s^2 + c^2 = 1$. 
Из этого свойства опорные элементы можно интерпретировать как синус и косинус от определённого угла $\phi$: $s = \sin \phi, c = \cos \phi$.
Матрица $C_{lm}$ является ортогональной, так как $C_{lm}^T C_{lm} = C_{lm} C_{lm}^T = E$.

## Типичный шаг метода вращений
Находим максимальный по модулю элемент матрицы $A$ вне главной диагонали и зафиксируем его индексы $l, m \ (l \ne m)$:
   $|a_{lm}| = \underset{i \ne j}\max |a_{ij}|$ 
   
Построим матрицу вращений, пока не определяя конкретные значения опорных элементов. 
Положим $B = C_{lm}^T A C_{lm}$, причём данная матрица будет являться симметричной:
$B^T = (C_{lm}^T A C_{lm})^T = C_{lm}^T A^T (C_{lm}^T)^T = C_{lm}^T A C_{lm} = B$ 
 
Получим формулы для опорных элементов. Для этого достаточно рассмотреть только верхнюю или нижнюю половину элементов от главной диагонали (т. к. матрица $B$ симметричная). 
Обозначим $D = A C_{lm}$.
Тогда $B = C_{lm}^T D$.

Так как у матрицы $C_lm$ отличаются только $l$-ый и $m$-ый столбцы от столбцов соответствующей единичной матрицы, то  $l$-ый и $m$-ый столбцы будут отличатся у матрицы $D$ от столбцов матрицы $A$.
$\begin{matrix}d_{il} = ca_{il} + sa_{im} \\ d_{im} = -sa_{il} + ca_{im}\end{matrix} \quad i = \overline{1, n}$

Так как у матрицы $C_{lm}^T$ только $l$-ая и $m$-ая строки отличаются от соответствующих строк единичной матрицы, то у матрицы $B$ будут отличатся только $l$-ая и $m$-ая строки от соответствующих строк матрицы $D$:
$\begin{matrix}b_{li} = cd_{li} + sd_{mi} \\ b_{mi} = -sd_{li} + cd_{mi}\end{matrix} \quad i = \overline{1, n}$

При $i \ne l$ и $i \ne m \quad d_{li} = a_{li}, d_{mi} = a_{mi}$, следовательно
$\begin{matrix}b_{li} = ca_{li} + sa_{mi} \\ b_{mi} = -sa_{li} + ca_{mi}\end{matrix} \quad i = \overline{1, n}, i \ne l, i \ne m$

При $i = l$ и учитывая, что $a_{lm} = a_{ml}$ получаем:
$b_{ll} = cd_{ll} + sd_{ml} = c(ca_{ll} + sa_{lm}) + s(ca_{ml} + sa_{mm}) = c^2a_{ll} + 2sca_{lm} + s^2a_{mm}$ 

При $i = m$ имеем:
$b_{lm} = cd_{lm} + sd_{mm} = c(-sa_{ll} + ca_{lm}) + s(-sa_{ml} + ca_{mm}) = sc(a_{mm} - a_{ll} + (c^2 - s^2)a_{lm})$
$b_{mm} = -sd_{lm} + cd_{mm} = -s(-sa_{ll} + ca_{lm}) + c(-sa_{ml} + ca_{mm}) = s^2a_{ll} + -2sca_{lm} + c^2a_{mm}$ 

Ввиду симметричности $B$ формулы выше полностью определяют эту матрицу.

Зададим опорные элементы так, чтобы $b_{lm} = 0$.
Тогда:
$sc(a_{mm} - a_{ll}) + (c^2 - s^2)a_{lm} = 0$
Подставив $c = \cos \phi, s = \sin \phi$ получим:
$\sin \phi \cos \phi (a_{mm} - a_{ll}) = -(\cos^2 \phi - \sin^2 \phi) a_{lm}$
$\dfrac{1}{2}(a_{ll} - a_{mm})\sin 2\phi = a_{lm}\cos 2\phi$
$\tan 2\phi = \dfrac{2a_{lm}}{a_{ll} - a_{mm}} \implies \phi = \dfrac{1}{2}\arctan \left(\dfrac{2a_{lm}}{a_{ll} - a_{mm}}\right)$

Для определённости будем считать $-\dfrac{\pi}{2} \lt 2\phi \le \dfrac{\pi}{2} \implies -\dfrac{\pi}{4} \lt \phi \le \dfrac{\pi}{4}$ 
(и $\phi = \dfrac{\pi}{4}$ при $a_{ll} = a_{mm}$).


# 6. Метод вращения решения проблемы собственных значений. Доказательство сходимости метода

Для доказательства сходимости метода потребуется следующее утверждение:
$$\underset{i \ne j}\sum b_{ij}^2 \le \left(1 - \dfrac{2}{n(n-1)}\right) \underset{i \ne j}\sum a_{ij}^2 \tag{1}$$

При умножении на ортогональную матрицу её евклидова норма не меняется, т. е.
$||B||_E = ||C_{lm}^T AC_{lm}||_E = ||A||_E \implies \sum\limits_{i=1}^n \sum\limits_{j=1}^n b_{ij}^2 = \sum\limits_{i=1}^n \sum\limits_{j=1}^n a_{ij}^2$
Перепишем это равенство в виде:
$\sum\limits_{i \ne j} b_{ij}^2 + \sum\limits_{i=1}^n b_{ii}^2 = \sum\limits_{i \ne j} a_{ij}^2 + \sum\limits_{i=1}^n a_{ii}^2$
$\sum\limits_{i \ne j} b_{ij}^2 - \sum\limits_{i \ne j} a_{ij}^2 = \sum\limits_{i=1}^n a_{ii}^2 - \sum\limits_{i=1}^n b_{ii}^2$
Так как все диагональные элементы, кроме $b_{ll}, b_{mm}$, совпадают с элементами матрицы $A$, то
$\sum\limits_{i \ne j} b_{ij}^2 - \sum\limits_{i \ne j} a_{ij}^2 = a_{ll}^2 + a_{mm}^2 - b_{ll}^2 - b_{mm}^2 \quad\quad (2)$

Введём в рассмотрение матрицы
$\overline A = \begin{pmatrix}a_{ll} & a_{lm}\\ a_{ml} & a_{mm}\end{pmatrix}, \overline C =  \begin{pmatrix}c & -s\\ s & c\end{pmatrix}, \overline B =  \begin{pmatrix}b_{ll} & 0\\ 0 & b_{mm}\end{pmatrix}$ 

учитывая формулы для $b_{ll}, b_{mm}$ и $b_{lm} = b_{ml} = 0$. Тогда нетрудно проверить, что $\overline B = \overline{C}^T \overline{A} \overline{C}$.  
Так как $\overline C$ ортогональна, то 
$||\overline B||_E = ||\overline A||_E \implies a_{ll}^2 + 2a_{lm}^2 + a_{mm}^2 = b_{ll}^2 + b_{mm}^2$

Теперь формула $(2)$ приобретает вид:
$\sum\limits_{i \ne j} b_{ij}^2 - \sum\limits_{i \ne j} a_{ij}^2 = -2a_{lm}^2 \quad\quad (3)$

Так как $|a_{lm}| = \max\limits_{i \ne j} |a_{ij}|$, то получаем оценку:
$\sum\limits_{i \ne j} a_{ij}^2 \le \sum\limits_{i \ne j} a_{lm}^2 = n(n - 1)a_{lm}^2$
Отсюда $-2a_{lm}^2 \le \dfrac{2}{n(n-1)} \sum\limits_{i \ne j} a_{ij}$.
Из $(3)$ следует: $\sum\limits_{i \ne j} b_{ij}^2 \le \left(1 - \dfrac{2}{n(n-1)}\right) \sum\limits_{i \ne j} a_{ij}^2$.
ч. т. д.


Рассмотрим метод вращений в целом. 
На первом шаге находим индексы $l_1, m_1$ максимального по модулю наддиагонального элемента $A$, строим матрицу вращения $C_1 = C_{l_1 m_1}$ с определёнными выше опорными элементами, и вычисляем матрицу $A_1 = C_1^T AC_1 \quad (4)$.

Пусть в результате выполнения $k-1$ шагов мы получили матрицу $A_{k-1}$. 
Находим наибольший по модулю наддиагональный элемент матрицы $A_{k-1}$ с индексами $l_k, m_k$, строим матрицу вращения $C_k = C_{l_k m_k}$ и вычисляем $A_k = C_k^T A_{k-1}C_k$.
Выразим $A_k$ через $A$:
$A_k = C_k^T A_{k-1}C_k = C_k^T C_{k-1}^T A_{k-2}C_{k-1}C_k = \ldots = (C_1 \ldots C_k)^T A (C_1 \ldots C_k)$ 

Произведение $U_k = C_1 \ldots C_k$ ортогональных матриц ортогонально, потому $A_k$ подобна $A$, следовательно их собственные значения совпадают. Покажем, что при $k \to \infty$ $A_k$ стремится к диагональной матрице.
Согласно $(1)$ и $(4)$ имеем;
$\sum\limits_{i \ne j} (a^{(k)}_{ij})^2 \le \left(1 - \dfrac{2}{n(n-1)}\right) \sum\limits_{i \ne j} (a^{(k-1)}_{ij})^2 \le \ldots \le \left(1 - \dfrac{2}{n(n-1)}\right)^k \sum\limits_{i \ne j} a_{ij}^2$ 

В силу того, что $\left(1 - \dfrac{2}{n(n-1)}\right) < 1$ следует $\sum\limits_{i \ne j} (a^{(k)}_{ij})^2 \underset{k \to \infty}\longrightarrow 0$ 
Также это означает, что $A_k$ стремится к диагональной матрице, которую обозначим через $\Lambda$. Так как $A_k$ подобна $A$, то и $\Lambda$ подобна $A$, т. е. на диагонали $\Lambda$ стоят собственные значения $A$.


# 7. Метод вращения решения проблемы собственных значений. Оценка точности приближений

Через $\Lambda_k$ обозначим матрицу из диагональных элементов $A_k$ (все остальные элементы равны нулю), а через $\epsilon_k$ обозначим $\epsilon_k = A_k - \Lambda_k$.
Пусть собственные значения матриц $A_k$ и $\Lambda_k$ пронумерованы в порядке неубывания. Тогда согласно следствию 2 из вариационного принципа Куранта-Фишера ([[#Следствие 2]]) имеют место оценки:
$|\lambda_i(A_k) - \lambda_i(\Lambda_k)| \le ||\epsilon_k||_2 \le ||\epsilon_k||_E, \quad i = \overline{1,n}$

Но $\lambda_i(A_k) = \lambda_i(A), \lambda_i(\Lambda_k) = a_{ii}^{(k)}$. Следовательно, 
$|\lambda_i(A) - a_{ii}^{(k)}| \le ||\epsilon_k||_E = \sqrt{\sum\limits_{i \ne j} (a_{ij}^{(k)})^2}$ 

Отсюда следует, что для определения собственных значений с точностью $\epsilon$ процесс следует продолжать, пока при некотором $k$ не выполнится неравенство: 
$\sum\limits_{i\ne j} (a_{ij}^{(k)})^2 \le \epsilon^2$.

Так как $U^T_k A U_k = A_k \approx \Lambda_k$, то собственные векторы $A$ приближённо равны столбцам матрицы $U_k = C_1 \ldots C_k$. Поэтому для нахождения собственных векторов следует последовательно перемножать матрицы $C_i, i = \overline{1, k}$.   


# 8. Метод вращения решения проблемы собственных значений. Определение опорных элементов

В вопросе 5 опорные элементы определялись как синус и косинус от угла $\phi$:
$\tan 2\phi = \dfrac{2a_{lm}}{a_{ll} - a_{mm}}, \quad -\dfrac{\pi}{4} \lt \phi \le \dfrac{\pi}{4}$.

На практике такой метод не применяется (из-за вычисления тригонометрических функций через ряд Тейлора, что требует значительных затрат времени). Потому определим $c, s$ так, чтобы не было необходимости вычислять значений таких функций.

Обозначим $x = 2a_{lm}, y = a_{ll} - a_{mm}$. Тогда $\tan 2\phi = \dfrac{x}{y}, \quad -\dfrac{\pi}{4} \lt \phi \le \dfrac{\pi}{4}$.
Если $y = 0$, то $\tan 2\phi = \infty$ и, следовательно, $2\phi = \dfrac{\pi}{2}$. Поэтому положим $c = \cos \dfrac{\pi}{4} = \dfrac{\sqrt{2}}{2}, s = \sin \dfrac{\pi}{4} = \dfrac{\sqrt{2}}{2}$.

Пусть $y \ne 0$. Представим $\tan 2\phi$ в виде
$$\tan 2\pi = \dfrac{x}{y} = \dfrac{\dfrac{sign(xy)|x|}{\sqrt{x^2 + y^2}}}{\dfrac{|y|}{\sqrt{x^2 + y^2}}} = \dfrac{\overline{s}}{\overline{c}} \tag{1}$$
где $\overline{s} = \dfrac{sign(xy)|x|}{\sqrt{x^2 + y^2}}, \overline{c} = \frac{|y|}{\sqrt{x^2 + y^2}}$

Поскольку $\overline{c}^2 + \overline{s}^2 = 1$, то $\overline c$ и $\overline s$ также можно рассматривать как синус и косинус некоторого угла: $\overline{c} = \cos \psi, \overline s = \sin \psi$.

Из $(1)$ следует, что $\tan 2\phi = \tan \psi$.
Откуда $\psi = 2\phi + k\pi, k = 0, 1, \ldots \quad\quad (2)$

Так как $-\dfrac{\pi}{2} \lt 2\phi \le \dfrac{\pi}{2}$, то $\cos 2\phi \le 0$. Поскольку $\cos \psi \le 0$, то в формуле $(2)$ следует оставить только ЧЕТЫРЕ значения $k$. Таким образом, $\psi = 2\psi + 2\pi k, k = 0, 1, \ldots$
а, следовательно $\cos \psi = \cos 2\phi, \sin \psi = \sin 2\phi$

Так как $\cos 2\phi = 2\cos^2 \phi - 1 = 2c^2 - 1, \sin 2\phi = 2\sin \phi \cos \phi = 2sc$, то мы получим два уравнения:
$2c^2 - 1 = \dfrac{|y|}{\sqrt{x^2 + y^2}}, 2sc = \dfrac{sign(xy)|x|}{\sqrt{x^2 + y^2}}$ 

Из первого из них получаем $c = \sqrt{\dfrac{1}{2} \left(1 + \dfrac{|y|}{\sqrt{x^2 + y^2}}\right)}$,
и второго $s = \dfrac{sign(xy)|x|}{2c\sqrt{x^2 + y^2}}$.


# 9. $QR$ - алгоритм решения проблемы собственных значений. Формулировка теоремы о сходимости

Пусть матрица $A$ невырожденная, тогда её можно представить в виде $A = QR$, $Q$ - ортогональная, $R$ - верхнетреугольная (невырожденная).

$QR$-алгоритм состоит из построения последовательности матриц $A_k$ по правилу:
$\begin{matrix}A = Q_1 R_1 &, & A_1 = R_1 Q_1, \\ A_1 = Q_2 R_2 &, & A_2 = R_2 Q_2,\\ \ldots && \ldots \\ A_{k-1} = Q_k R_k &, & A_k = R_k Q_k, \\ \ldots && \ldots\end{matrix} \quad\quad (1)$

, где все $Q_i$ ортогональны, а $R_i$ невырожденные верхнетреугольные матрицы.

Для всякой матрицы $A$ существует такая невырожденная матрица $T$, что матрица
$\Lambda = T^{-1} AT \quad\quad (2)$
имеет нормальную жорданову форму, т. е. $\Lambda$ является блочно-диагональной матрицей:
$$\Lambda = \begin{pmatrix}\mathcal J_1 &&&0 \\ & \mathcal J_2 \\ && \ldots \\ 0 &&& \mathcal J_m \end{pmatrix}$$

, где каждый диагональный блок $\mathcal J_i$ является двухдиагональной матрицей:
$$\mathcal{J}_i = \begin{pmatrix}\lambda_i(A) &1&&0&0 \\ & \lambda_i(A) & 1 && 0 \\ && \ldots & \\ 0 &&& \lambda_i(A) & 1 \\ 0 & 0 &&& \lambda_i(A) \end{pmatrix}$$
причём её диагональные элементы равны одному из собственных значений матрицы $A$.

Представим матрицу $A_k$ в клеточном виде:
$$A_k = \begin{pmatrix}A_{11}^{(k)} & A_{12}^{(k)} & \ldots & A_{1m}^{(k)}\\ A_{21}^{(k)} & A_{22}^{(k)} & \ldots & A_{2m}^{(k)}\\ \cdot & \cdot & \ldots & \cdot\\ A_{m1}^{(k)} & A_{m2}^{(k)} & \ldots & A_{mm}^{(k)}\end{pmatrix}$$
где диагональные клетки $A_{ii}^{(k)}$ квадратные и имеют те же размеры, что и соответствующие диагональные блоки $\mathcal J_i$ матрицы $\Lambda$.

***Утверждение (без доказательства)***
	При $k \to \infty$ все элементы поддиагональных клеток стремятся к нулю, а элементы всех остальных клеток остаются ограниченными.

Пусть все собственные значения матрицы $A$ вещественные и различны по модулю. Предположим, что они занумерованы в порядке убывания:
$|\lambda_i(A)| > |\lambda_2(A)| > \ldots > |\lambda_n(A)| > 0$

Нормальная жорданова форма форма матрицы, все собственные значения которой различны, является диагональной матрицей. Поэтому матрица $\Lambda$ - диагональная. Предположим, что её диагональные элементы также упорядочены в порядке убывания их модулей, т. е. $\Lambda$ имеет вид:
$$\Lambda = \begin{pmatrix}\lambda_1(A) &&&0 \\ & \lambda_2(A) \\ && \ldots \\ 0 &&& \lambda_n(A) \end{pmatrix}$$


#### Теорема
Пусть все собственные значения невырожденной матрицы $A$ вещественны и различны по модулю, и пусть все главные миноры матрицы $T^{-1}$ из равенства $(2)$ отличны от нуля. Тогда последовательность матриц $A_k$, определённых по правилу $(1)$, сходится к верхнетреугольной матрице. 
	*(Без доказательства)*

Матрицы вида:
$$H = \begin{pmatrix}h_{11} & h_{12} & \ldots & h_{1, n-1} & h_{1n}\\ h_{21} & h_{22} & \ldots & h_{2, n-1} & h_{2n}\\ & h_{32} & \ldots & h_{3, n-1} & h_{3n}\\ 0 && \ldots & \vdots & \vdots\\ 0 & 0 && h_{n,n-1} & h_{nn}\end{pmatrix}$$
Называются верхними почти треугольными или верхними матрицами Хессенберга. Всякую квадратную матрицу можно привести к подобной ей верхней форме Хессенберга.

$QR$-алгоритм применяют, как правило, к матрице, предварительно приведённой к верхней форме Хессенберга. Связано это с многократным вычислением $QR$-разложения для матриц. Но $QR$-разложение матрицы Хессенберга осуществляется за $O(n^2)$ арифметических действий, причём форма Хессенберга инвариантна по отношению к преобразованиям $QR$-алгоритма (т. е. если $A$ - матрица Хессенберга, то и $A_k$ - матрица Хессенберга). 


# 10. Решение частичной проблемы собственных значений. Метод прямых итераций. Метод обратных итераций
Пусть у вещественной матрицы $A$ все собственные значения вещественны и различны по модулю. Пусть $\left| \lambda_{1}(A) \right|$ - наибольшее по модулю СЗ.
## Прямая итерация
### Метод
$x^{(k)} = \dfrac{Ax^{(k - 1)}}{\alpha_{k - 1}}$
$\lambda_{1}(A) = \dfrac{\left( Ax^{(k)}, x^{(k)} \right)}{(x^{(k)}, x^{(k)})} = \dfrac{(x^{(k)})^{T}A^{(k)}x^{(k)}}{(x^{(k)})^{T}x^{(k)}}$
### Доказательство
Выберем произвольный вектор $x^{(0)}$ и построим последовательность векторов так, что $x^{(k)} = Ax^{(k - 1)}$.
Очевидно, что $x^{(k)} = A^{k}x^{(0)}$.
Представим:
$x^{(0)} = \sum \limits_{ i = 1 }^{ n }c_{i}u^{(i)}$
Тогда: 
$x^{(k)} = A^{k}x^{(0)} = A^{k}\sum \limits_{ i = 1 }^{ n }c_{i}u^{(i)} = \sum \limits_{ i = 1 }^{ n }c_{i}A^{k}u^{(i)} = \sum \limits_{ i = 1 }^{ n }c_{i}\lambda_{i}^{k}(A)u^{(i)} =$
$= \lambda_{1}^{k}(A)\left[ c_{1}u^{(1)} + \sum \limits_{ i = 2 }^{ n }c_{i}u^{(i)}\left( \dfrac{\lambda_{i}(A)}{\lambda_{1}(A)} \right)^{k} \right] = \lambda_{1}^{k}(A)\cdot v^{(k)}$
Так как $\left| \dfrac{\lambda_{i}(A)}{\lambda_{1}(A)} \right| < 1$, $\lim \limits_{ k \to \infty }v^{(k)} = c_{1}u^{(1)}$

Пусть $c_{1} \neq 0$.
Тогда вектор $c_{1}u^{(1)}$ отличается от $u^{(1)}$ только константой и будет собственным вектором, отвечающим наибольшему по модулю собственному значению $\lambda_{1}$.
Вектор $x^{(k)}$ отличается от $v^{(k)}$ лишь множителем $\lambda_{k}(A)$, тогда $x^{(k)} \to$ к собственному вектору при $k \to \infty$

Проблемы возникают из-за переполнений при вычислении вектора $x^{(k)}$.
Для этого будем нормировать вектор:
$\alpha_{k} = \underset{ 1 \leq i \leq n }{ \max }\left| x_{i}^{(k)} \right|$
$x^{(k)} = A\dfrac{x^{(k - 1)}}{\alpha_{k - 1}}$
Это не нарушит сходимость последовательности к СВ, и более того, так как $\underset{ 1 \leq i \leq n }{ \max }\left| \dfrac{x_{i}^{(k)}}{\alpha_{k}} \right| = 1$, то $\left| \dfrac{x_{i}^{(k)}}{\alpha_{k}} \right| \to$ к СВ, имеющему наибольшую по модулю компоненту, равную 1.
Таким СВ будет является $\dfrac{u^{(1)}}{\alpha}$, где $\alpha = \underset{ 1 \leq i \leq n }{ \max }\left| u^{(1)}_{i} \right|$
Следовательно:
$\left| \dfrac{x^{(k)}}{\alpha_{k}} \right|\to \dfrac{u^{(1)}}{\alpha}$ и $\dfrac{x^{(k - 1)}_{i}}{\alpha_{k - 1}}\to\dfrac{u^{(1)}}{\alpha}$
Но $A\dfrac{x^{(k - 1)}}{\alpha_{k - 1}} \to A\dfrac{u^{(1)}}{\alpha} = \dfrac{\lambda_{k}(A)u^{(1)}}{\alpha}$
Но так как $\dfrac{Ax^{(k - 1)}}{\alpha_{k - 1}}=x^{(k)}$, тогда $x^{(k)}\to\lambda(A)\dfrac{u^{(1)}}{\alpha}$
Следовательно $\alpha_{k}$ стремится к наибольшей по модулю компоненте вектора $\lambda_{1}(A) \dfrac{u^{(1)}}{\alpha}$, равной $\lambda_{1}(A)$.
Следовательно, $\alpha_{k} \to \lambda_{1}(A)$

## Обратная итерация
### Метод
$x^{(k)} = A^{-1}\dfrac{x^{(k - 1)}}{\alpha_{k - 1}}$
$Ax^{(k)} = \dfrac{x^{(k - 1)}}{\alpha_{k - 1}}$
### Доказательство
Поскольку СЗ обратной матрицы обратны собственным значениям исходной матрицы, а собственные вектора совпадают с собственными векторами исходной матрицы, то последовательной $x^{(k)}$ будет сходиться к СВ исходной матрицы $A$, отвечающему её наименьшему по модулю значению, при этом $\alpha_{k} \to \dfrac 1 {\lambda_{n}(A)} \implies \dfrac{1}{\alpha_{k}} \to \lambda_{k}(A)$.
Следовательно, метод обратных итераций позволяет вычислить наименьшее по модулю СЗ исходной матрицы, и отвечающий ему СВ.