
# 1.	Подчиненные матричные нормы. Свойства. Формула для вычисления второй нормы матрицы.

**Подчинённая матричная норма**
	Норма, которая вычисляется по формуле $\|A\| = \underset{x \neq 0}{\sup} \dfrac{\|A{x}\|}{\|x\|}$
**Свойства**
1. $\|E\| = 1$
		$\| E \|=\underset{x \neq 0}{\sup} \dfrac{\|E{x}\|}{\|x\|} = \underset{x \neq 0}{\sup} \dfrac{\|x\|}{\|x\|}=1$
2. $\|AB\| \le \|A\| \cdot \|B\|$
		$\|AB\| = \underset{x \neq 0}{\sup} \dfrac{\|AB{x}\|}{\|x\|}\le \underset{x \neq 0}{\sup} \dfrac{\|A\|\cdot\|B{x}\|}{\|x\|}=\|A\|\cdot\|B\|$

**Вторая матричная норма**
	$||A||_{2} = \sqrt{\max\limits_{i} \lambda_{i}(A^T A)}$

# 2.	Подчиненные матричные нормы. Свойства. Формула для вычисления третьей нормы матрицы.

**Подчинённая матричная норма**
	Норма, которая удовлетворяет условию $||A|| = \underset{x \neq 0}{\sup} \dfrac{||A{x}||}{||x||}$
	То есть, если матричная норма подчинена введённой до неё векторной норме
	Любая подчинённая матричная норма согласованна. Обратное — не факт.
	*Подчинение - более сильная форма согласования*
**Свойства**
1. $\|E\| = 1$
		$\| E \|=\underset{x \neq 0}{\sup} \dfrac{\|E{x}\|}{\|x\|} = \underset{x \neq 0}{\sup} \dfrac{\|x\|}{\|x\|}=1$
2. $\|AB\| \le \|A\| \cdot \|B\|$
		$\|AB\| = \underset{x \neq 0}{\sup} \dfrac{\|AB{x}\|}{\|x\|}\le \underset{x \neq 0}{\sup} \dfrac{\|A\|\cdot\|B{x}\|}{\|x\|}=\|A\|\cdot\|B\|$

**Третья матричная норма**
	$||A||_{\infty} = \max_{i} \sum \limits_{j = 1}^n |a_{ij}|$

# 3.	Согласованные матричные нормы.

**Матричная норма согласованная с векторной**
	Норма для которой выполняется неравенство $||Ax||\le||A||\cdot||x||\quad \forall x$

# 4.	Число обусловленности матрицы. Определение. Вычислительная формула. Пример плохо обусловленной системы.

**Число обусловленности матрицы $A$**.
	Также записывается как $\text{cond}(A)$
	Число, показывающее, насколько может измениться значение функции при небольшом изменении аргумента.
	$\mu(A) = \underset{ y \neq 0 }{ \underset{ x \neq 0 }{ sup } }\left(\dfrac{||Ax||}{||x||} / \dfrac{||Ay||}{||y||}\right)$
	$\mu(A) \geq 1$

**Вычислительная формула**
	$\mu(A) = ||A||\cdot ||A^{-1}||$

**Вывод**
	$\mu(A) = \underset{ y \neq 0 }{ \underset{ x \neq 0 }{ sup } }\left(\dfrac{||Ax||}{||x||} / \dfrac{||Ay||}{||y||}\right) = (\underset{ x \neq 0 }{ \sup }\dfrac{||Ax||}{||x||} / \underset{ y \neq 0 }{ \inf }\dfrac{||Ay||}{||y||})=$
	$z=Ay\implies y=A^{-1}z$
	$=\dfrac{||A||}{\inf\limits_{z\neq 0}\frac{||z||}{||A^{-1}z||}}=\dfrac{||A||}{\dfrac{1}{\sup\limits_{z\neq 0}\frac{||A^{-1}z||}{||z||}}}=\dfrac{||A||}{\frac{1}{||A^{-1}||}}=||A||\cdot ||A^{-1}||$

**Плохо обусловленная матрица**
	$A=\begin{pmatrix}\mathsf{1}&&\mathsf{1}\\\mathsf{1}&&\mathsf{1.01}\end{pmatrix}$
	$||A|| = 1 + 1.01$
	$||A^{-1}|| = \left|\left\|\begin{pmatrix}101 & -100 \\ -100 & 100\end{pmatrix}\right|\right| = 201$
	$||A|| \cdot ||A^{-1}|| = 404.01$
	Как видим, при небольшом изменении нашей начальной матрицы, мы получили большое изменение результата, что означает, что матрица плохо обусловлена

# 5.	Метод Гаусса.
## Описание метода
Пусть имеется некоторая система
$$
\begin{cases}
a_{11}x_{1} + \ldots + a_{1n}x_{n} = b_{1} \\
\cdots \\
a_{m_{1}}x_{1} + \ldots + a_{mn}x_{n} = b_{m}
\end{cases}
$$
С помощью элементарных преобразований приведём её к ступенчатому виду (aka прямой ход метода Гаусса)
$$
\begin{cases}
\alpha_{1j_{1}}x_{j_{1}} &  + \alpha_{1j_{2}}x_{j_{2}} &  + \ldots &  + \alpha_{1j_{r}}x_{j_r} &  + \ldots &  + \alpha_{1j_{n}}x_{j_{n}} &  = \beta_{1} \\
0 & +\alpha_{2j_{2}}x_{j_{2}} & +\ldots & +\alpha_{2j_{r}}x_{j_{r}} & +\ldots & + \alpha_{1j_{n}}x_{j_{n}} &  = \beta_{2} \\
\cdots \\
0 & +0 & +\ldots & +\alpha_{rj_{r}}x_{j_{r}} &  + \ldots & + \alpha_{rj_{n}}x_{j_{n}} &  = \beta_{r} \\
\ldots \\
0 & +0 & +\ldots & +0 & +\ldots & +0 &  = \beta_{r + 1} \\
\ldots \\
0 & +0 & +\ldots & +0 & +\ldots & +0 &  = \beta_{m}
\end{cases}
$$

Пусть $\forall i \geq r + 1 \quad \beta_{i} = 0$
Перенесём свободные переменные за знак равенств и поделим каждое из уравнений системы на свой коэффициент при самом левом $x$
$$
\begin{matrix} \\
\begin{cases}
x_{j_{1}} &  + \hat{ \alpha }_{1j_{2}}x_{j_{2}} &  + \ldots &  + \hat{ \alpha }_{1j_{r}} x_{j_{r}} &  = &  \hat{ \beta }_{1} &  - \hat{ \alpha }_{1j_{r + 1}} x_{j_{r + 1}} &  - \ldots &  - \hat{ \alpha }_{1j_{n}}x_{j_{n}} \\
0 & +x_{j_{2}} & +\ldots & +\hat{ \alpha }_{2j_{r}}x_{j_{r}} &  = & \hat{ \beta }_{2} &  - \hat{ \alpha }_{2j_{r + 1}}x_{j_{r + 1}} &  - \ldots & \hat{ -\alpha }_{2j_{n}}x_{j_{n}} \\
 &  &  &  & \cdots \\
0 &+0 &+0  &  +x_{j_{r}} & = & \hat{ \beta }_{r} & -\hat{ \alpha }_{rj_{r + 1}}x_{j_{r + 1}} &  - \ldots & -\hat{ \alpha }_{rj_{n}}x_{j_{n}}
\end{cases} \\
\hat{\beta_{i}} = \frac{\beta_{i}}{\alpha_{ij_{i}}} \\
\hat{ \alpha }_{ij_{k}} = \frac{\alpha_{ij_k}}{\alpha_{ij_{i}}} \\
i = 1, \ldots, r \\
k = i + 1, \ldots, n 
\end{matrix}
$$


**Замечание**
	После прямого хода метода Гаусса можно легко вычислить определитель $|A|=\prod\limits_{i=1}^{n}a_{ii}$
	(Лол)

# 6.	LU – разложение. Необходимое и достаточное условие существования. (ПРАВКА)

**LU - разложение**
	Декомпозиция матрицы системы $A$ на две треугольные матрицы $L, U$ так, что бы $A=L\cdot U$
	$$A=
	\begin{pmatrix}
	{a_{11}} && {a_{12}} && {\ldots} && {a_{1n}} \\
	{a_{21}} && {a_{22}} && {\ldots} && {a_{2n}} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{a_{n1}} && {a_{n2}} && {\ldots} && {a_{nn}} 
	\end{pmatrix}$$
	$$
	\quad L =
	\begin{pmatrix}
	{1} && {0} && {\ldots} && {0} \\
	{l_{21}} && {1} && {\ldots} && {0} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{l_{n1}} && {l_{n2}} && {\ldots} && {1} 
	\end{pmatrix}
	\quad U=
	\begin{pmatrix}
	{u_{11}} && {u_{12}} && {\ldots} && {u_{1n}} \\
	{0} && {u_{22}} && {\ldots} && {u_{2n}} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{0} && {0} && {\ldots} && {u_{nn}} 
	\end{pmatrix} 
	$$
	Дальше по правилам умножения матриц выводятся общие формулы элементов
	*Если $i\ge j:$
		$u_{ij}=a_{ij}-\sum\limits_{k=1}^{i} l_{ik}\cdot u_{kj}$
	Если $i<j:$*
		$l_{ij}=\dfrac{a_{ij}-\sum\limits_{k=1}^j l_{ik}\cdot u_{kj} }{u_{jj}}$

**Вывод формул**
	$$
	D_{1} = \begin{pmatrix}
	1 & 0 & \ldots & 0\\
	-d_{21} & 1 & \ldots & 0 \\
	\ldots & \ldots & \ldots & \ldots \\
	-d_{n1} & 0 & \ldots & 1
	\end{pmatrix}  \quad 
	
	D_{2} = \begin{pmatrix}
	1 & 0 & \ldots & 0\\
	0 & 1 & \ldots & 0 \\
	\ldots & \ldots & \ldots & \ldots \\
	0 & -d_{n2} & \ldots & 1
	\end{pmatrix}  \quad 
	\cdots  \quad  
	D_{n - 1}
	$$
	$$A_i=D_iA_{i-1}$$
	$$U=A_{n-1}=D_{n-1}A_{n-2}=...=D_{n-1}D_{n-2}...D_1A=DA$$
	$$D^{-1}=L$$
	$$A=LU$$
	$$
	L = \begin{pmatrix}
	1 & 0 & 0 & \ldots & 0 \\
	d_{21} & 1 & 0  & \ldots & 0 \\
	d_{21}d_{32} + d_{31} & d_{32} & 1 & \ldots & 0 \\
	\ldots & \ldots & \ldots & \ldots & \ldots \\
	d_{n1} & d_{n2} & d_{n3} & \ldots & 1
	\end{pmatrix}
	$$
	Крч, тут матрица не совсем правильная, ибо из-за инверсии всё начинает рушится, так что читателю предлагается исправить матрицу в качестве несложного домашнего упражнения

**Необходимое и достаточное условие существования**
	Все угловые миноры матрицы $A$ невырожденные

**Замечание**
	После LU разложения можно легко вычислить определитель $|A|=\prod\limits_{i=1}^{n}u_{ii}$

**Применение**
	Предположим, у нас есть система $Ax = b$
	Тогда, $Ly  = b, \quad Ux = y$
	Из этих уравнений можно проще высчитать наши $x$.
# 7.	Метод квадратного корня (a.k.a Метод Холецкого).
**Метод квадратного корня**
	Частный случай $LU$ разложения для симметричных матриц $(A=A^T)$
		$A = \begin{pmatrix}a_{11} & a_{12} & \ldots & a_{1n} \\ a_{12} & a_{22} & \ldots & a_{2n} \\ \vdots & \vdots & \ddots & \cdots \\ a_{1n} & a_{2n} & \ldots & a_{nn} \end{pmatrix}$
	$A=LL^{T}$ или $A = U^{T}U$
	Разберём на примере $A = LL^T$
	$l_{11} = \sqrt{ a_{11} }$
	$l_{j1} = \frac{a_{j1}}{l_{11}}$
	$l_{ii} = \sqrt{ a_{ii} - \underset{ p = 1 }{ \overset{ i - 1 }{ \sum } }l^{2}_{ip} }, \quad i \in \left[ 2, n \right]$
	$l_{ji} = \frac{1}{l_{ii}}\left( a_{ji} - \underset{ p = 1 }{ \overset{ i - 1 }{ \sum } }l_{ip}l_{jp} \right), \quad i \in \left[ 2, n - 1 \right],\ j \in \left[ i + 1, n \right]$

**Вывод формул**
		$A=LU\quad U = DV$
		$D = \begin{pmatrix}d_1 & 0 & \ldots & 0 \\ 0 & d_2 & \ldots & 0 \\ \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & \ldots & d_n\end{pmatrix}\quad V = \begin{pmatrix}1 & v_{12} & \ldots & v_{1n} \\ 0 & 1 & \ldots & v_{2n} \\ \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & \ldots & 1\end{pmatrix}$
		$d_{i} = u_{ii}\quad v_{ij} = \dfrac{u_{ij}}{u_{ii}}$
		
		$A = LDV$
		$A^{T} = (LDV)^{T} = V^{T}D^{T}L^{T} = V^{T}DL^{T}=A$
		$L = V^{T}\quad V = L^{T}$
		$A = V^{T}DV$
		$D^{1/2} = \begin{pmatrix}\sqrt{d_1} & 0 & \ldots & 0 \\ 0 & \sqrt{d_2} & \ldots & 0 \\ \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & \ldots & \sqrt{d_n}\end{pmatrix}$
		$A = V^{T}D^{1/2}D^{1/2}V$
		$A = W^{T}W\quad W=D^{1\backslash 2}V$
		
		$a_{ij} = \sum \limits_{ k = 1 } ^{ n } w^{T}_{ik}w_{kj} = \sum \limits_{ k = 1 } ^{ i }w_{ki}w_{kj} = a_{ij}$
		$a_{11} = w_{11} w_{11}$
		$w_{11} = \sqrt{ a_{11 }}$
		$a_{1j} = w_{11}w_{1j}  \quad   \quad w_{1j} = \dfrac{a_{1j}}{w_{11}}$
		
		$w_{ii} = \sqrt{ a_{ii} - \sum \limits_{ k=1 }^{ i - 1 }w_{ki}^{2} }$
		$w_{ij} = \dfrac{a_{ii} - \sum \limits_{ k=1 }^{ i - 1 }w_{ki}w_{kj}}{w_{ii}}  \quad   \quad j = i + 1, \ldots, n$

	

# 8.	Матрица отражения, ее свойства.
**Определение**
	Пусть $p\in \mathbb{R}^n-$ вектор нормали некоторой гиперплоскости в $\mathbb{R}^n$, проходящей через $\theta$
	Тогда $y=x-2\dfrac{(p, x)}{(p, p)}p~-$ задает вектор $y$, отраженный к $x$ относительно гиперплоскости $(p, x)=0$
	Проведем ряд преобразований:
	$y=x-2\dfrac{(p, x)}{(p, p)}p=x-\dfrac{2}{(p, p)}\cdot(p,x)\cdot p=x-\dfrac{2}{(p, p)}\cdot p \cdot p^T \cdot x=(E-\dfrac{2}{(p, p)}\cdot p \cdot p^T)\cdot x$
	Обозначим $P=E-\dfrac{2}{(p, p)}\cdot p \cdot p^T$
	Тогда $P~-$ матрица отражения

**Свойства**
1. $P^2=E$
2. $P^T=P$
3. $P^T\cdot P = P^2=E\implies P-$ ортогональная матрица
4. При замене нормали $p=\beta\cdot p$ матрица отражения не изменится
5. $p=x-y$
6. $p_i=0\implies y_i = x_i \quad i=\overline{1,k}$
7. $\begin{cases}{p_1=\ldots=p_k=0}\\ {x_{k+1}=\ldots=x_n=0}\end{cases}\implies y_{k+1}=\ldots=y_n=0$

# 9.	QR – разложение. Условие существования QR – разложения.
**$QR$ разложение**
	$A = QR$
	$A_{1} = P_{1}A$
	$\begin{pmatrix}a_{11}\\ 0\\ \ldots\\ 0\end{pmatrix} = a_{1}^{(1)}  \quad \begin{pmatrix}a_{11}\\ a_{21}\\ \ldots\\ a_{nn}\end{pmatrix}$ 
	
	$P_{1}a_{1} = \begin{pmatrix}a_{11}^{(1)}\\ 0\\ \ldots\\ 0\end{pmatrix}$
	
	$p_{1}^{(1)} = a_{11} - a_{11}^{(1)}$
	$p_{2}^{(1)} = a_{21}$
	$\ldots$
	$p_{n}^{(1)} = a_{n_{1}}$
	
	$||a_{1}^{(1)}||_{2}^{2} = ||a_{1}||_{2}^{2}$
	$(a_{11}^{(1)})^{2} = \sum \limits_{ l=1 }^{ n }a_{11}^{2}$
	$a_{11}^{(1)} = \pm \sqrt{ \sum \limits_{ l=1 }^{ n }a_{l1}^{2} }$
	
	$\mathfrak S = \begin{cases}1, & a_{11}\geq 0\\ -1, & a_{11}< 0 \end{cases}$
	
	$p_{1}^{(1)} = a_{11} + \mathfrak S_{1} \cdot \sqrt{ \sum \limits_{ l=1 }^{ n }a_{l_{1}}^{2} }$
	
	$a_{j}^{(1)} = a_{j} - 2 \frac{(p^{(1)}a_{j})}{(p^{(1)}, p^{(1)})}p^{(1)}  \quad j = 2, \ldots, n$



**Условие существования**
	Разложение есть всегда(если матрица невырожденая)

# 10.	Метод окаймления обращения матриц.
$A_j-$ матрица порядка $j$
Метод вычисления матрицы $A_i^{-1},$ при известной $A_{i-1}^{-1}$
$$A_i=\begin{pmatrix}
{A_{i-1}} && {a_{12}} \\
{a_{21}} && {a_{22}}
\end{pmatrix}\quad
A_i^{-1}=\begin{pmatrix}
{B_{i-1}} && {b_{12}} \\
{b_{21}} && {b_{22}}
\end{pmatrix}$$
$a_{12}, b_{12}-$ вектор столбец, $a_{21}, b_{21}-$ вектор строка, $a_{22}, b_{22}-$ числа
По определению обратной матрицы:
	$A_{i}\cdot A_{i}^{-1}=\begin{pmatrix} {E} && {\theta}\\ {\theta^T} && {1} \end{pmatrix}$
По правилам перемножения матриц получаем систему:
	$$
	\begin{cases}
	{A_{i-1}\cdot B_{i-1}+b_{12}\cdot a_{21}=E} \\
	{A_{i-1}\cdot b_{12}+a_{12}\cdot b_{22}=\theta} \\
	{a_{21}\cdot A_{i-1}^{-1}+a_{22}\cdot b_{21}=\theta^T} \\
	{a_{21}\cdot b_{12} + a_{22}\cdot b_{22}=1}
	\end{cases}
	$$
$$
\begin{cases}
{k=A_{i-1}^{-1}\cdot \left( k\cdot E+B\cdot C\cdot A_{i-1}^{-1} \right)} \\
{B_{i-1} = \dfrac{1}{k}\cdot(A_{i-1}-a_{12}\cdot a_{21} \cdot A_{i-1}^{-1})} \\
{b_{12}=-\dfrac{1}{k}\cdot A_{i-1}^{-1}\cdot B} \\
{b_{21}=-\dfrac{1}{k}\cdot a_{21}\cdot A_{i-1}^{-1}} \\
{b_{22}=\dfrac{1}{k}}
\end{cases}
$$
