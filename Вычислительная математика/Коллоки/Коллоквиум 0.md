Мы слишком молоды для этого дерьма (пока что)

## 1.	Подчиненные матричные нормы. Свойства. Формула для вычисления второй нормы матрицы.

**Подчинённая матричная норма**
	Норма, которая удовлетворяет условию $||A|| = \underset{x \neq 0}{\sup} \dfrac{||A{x}||}{||x||}$
**Свойства**
1. $||E|| = 1$
2. $||AB|| \le ||A|| \cdot ||B||$

**Вторая матричная норма**
$||A||_{2} = \sqrt{\max_{i} \lambda_{i} \sum \limits_{i = 1}^n A^T A}$

## 2.	Подчиненные матричные нормы. Свойства. Формула для вычисления третьей нормы матрицы.

**Подчинённая матричная норма**
	Норма, которая удовлетворяет условию $||A|| = \underset{x \neq 0}{\sup} \dfrac{||A{x}||}{||x||}$
**Свойства**
1. $||E|| = 1$
2. $||AB|| \le ||A|| \cdot ||B||$

**Третья матричная норма**
Просто распиши p норму, лол

## 3.	Согласованные матричные нормы.

**Согласованная матричная норма**
	$||Ax||\le||A||\cdot||x||$

## 4.	Число обусловленности матрицы. Определение. Вычислительная формула. Пример плохо обусловленной системы.

**Число обусловленности матрицы $A$**.
	Число, показывающее, насколько может измениться значение функции при небольшом изменении аргумента.
	$\mu(A) = \underset{ y \neq 0 }{ \underset{ x \neq 0 }{ sup } }\left(\dfrac{||Ax||}{||x||} / \dfrac{||Ay||}{||y||}\right)$

**Вычислительная формула**
	$\mu(A) = ||A||\cdot ||A^{-1}||$

**Вывод**
	$\mu(A) = \underset{ y \neq 0 }{ \underset{ x \neq 0 }{ sup } }\left(\dfrac{||Ax||}{||x||} / \dfrac{||Ay||}{||y||}\right) = (\underset{ x \neq 0 }{ \sup }\dfrac{||Ax||}{||x||} / \underset{ y \neq 0 }{ \inf }\dfrac{||Ay||}{||y||})=$
	$z=Ay\implies y=A^{-1}z$
	$=\dfrac{||A||}{\inf\limits_{z\neq 0}\dfrac{||z||}{||A^{-1}z||}}=\dfrac{||A||}{\dfrac{1}{\sup\limits_{z\neq 0}\dfrac{||A^{-1}z||}{||z||}}}=\dfrac{||A||}{\dfrac{1}{||A^{-1}||}}=||A||\cdot ||A^{-1}||$

**Плохо обусловленная матрица**
	$A=\begin{pmatrix}\mathsf{1}&&\mathsf{1}\\\mathsf{1}&&\mathsf{1.01}\end{pmatrix}$
	Оставлю вычисление $\mathsf{cond(A)}$ читателю в качестве несложного упражнения
	P. S. Там что-то около 400

## 5.	Метод Гаусса.

Ебал я это оформлять

## 6.	LU – разложение. Необходимое и достаточное условие существования.

**LU - разложение**
	Декомпозиция матрицы системы $A$ на две треугольные матрицы $L, U$ так, что бы $A=L\cdot U$
	$$
	\begin{pmatrix}
	{} && {} && {\ldots} && {} \\
	{} && {} && {\ldots} && {} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{} && {} && {\ldots} && {} 
	\end{pmatrix}
	$$

**Необходимое и достаточное условие существования**
	Что-то там что все главные миноры не 0, помоему
## 7.	Метод квадратного корня.
## 8.	Матрица отражения, ее свойства.
**Определение**
	Пусть $p\in R^n-$ вектор нормали некоторой гиперплоскости в $R^n$, проходящей через $\theta$
	Тогда $y=x-2\dfrac{(p, x)}{(p, p)}p~-$ задает вектор $y$, отраженный к $x$ относительно гиперплоскости $(p, x)=0$
	Проведем ряд преобразований:
	$y=x-2\dfrac{(p, x)}{(p, p)}p=x-\dfrac{2}{(p, p)}\cdot(p,x)\cdot p=x-\dfrac{2}{(p, p)}\cdot p \cdot p^T \cdot x=(E-\dfrac{2}{(p, p)}\cdot p \cdot p^T)\cdot x$
	Обозначим $P=E-\dfrac{2}{(p, p)}\cdot p \cdot p^T$
	Тогда $P~-$ матрица отражения

**Свойства**
1. $P^2=E$
2. $P^T=P$
3. $P^T\cdot P = P^2=E\implies P-$ ортогональная матрица
4. При замене нормали $p=\beta\cdot p$ матрица отражения не изменится
5. $p=x-y$
6. $p_i=0\implies y_i = x_i \quad i=\overline{1,k}$
7. $\begin{cases}{p_1=\ldots=p_k=0}\\ {x_{k+1}=\ldots=x_n=0}\end{cases}\implies y_{k+1}=\ldots=y_n=0$

## 9.	QR – разложение. Условие существования QR – разложения.
## 10.	Метод окаймления обращения матриц.
