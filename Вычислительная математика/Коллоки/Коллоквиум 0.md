
# 1.	Подчиненные матричные нормы. Свойства. Формула для вычисления второй нормы матрицы.

**Подчинённая матричная норма**
	Норма, которая вычисляется по формуле $\|A\| = \underset{x \neq 0}{\sup} \dfrac{\|A{x}\|}{\|x\|}$
**Свойства**
1. $\|E\| = 1$
		$\| E \|=\underset{x \neq 0}{\sup} \dfrac{\|E{x}\|}{\|x\|} = \underset{x \neq 0}{\sup} \dfrac{\|x\|}{\|x\|}=1$
2. $\|AB\| \le \|A\| \cdot \|B\|$
		$\|AB\| = \underset{x \neq 0}{\sup} \dfrac{\|AB{x}\|}{\|x\|}\le \underset{x \neq 0}{\sup} \dfrac{\|A\|\cdot\|B{x}\|}{\|x\|}=\|A\|\cdot\|B\|$

**Вторая матричная норма**
	$||A||_{2} = \sqrt{\max\limits_{i} \lambda_{i}(A^T A)}$

# 2.	Подчиненные матричные нормы. Свойства. Формула для вычисления третьей нормы матрицы.

**Подчинённая матричная норма**
	Норма, которая удовлетворяет условию $||A|| = \underset{x \neq 0}{\sup} \dfrac{||A{x}||}{||x||}$
	То есть, если матричная норма подчинена введённой до неё векторной норме
	Любая подчинённая матричная норма согласованна. Обратное — не факт.
	*Подчинение - более сильная форма согласования*
**Свойства**
1. $\|E\| = 1$
		$\| E \|=\underset{x \neq 0}{\sup} \dfrac{\|E{x}\|}{\|x\|} = \underset{x \neq 0}{\sup} \dfrac{\|x\|}{\|x\|}=1$
2. $\|AB\| \le \|A\| \cdot \|B\|$
		$\|AB\| = \underset{x \neq 0}{\sup} \dfrac{\|AB{x}\|}{\|x\|}\le \underset{x \neq 0}{\sup} \dfrac{\|A\|\cdot\|B{x}\|}{\|x\|}=\|A\|\cdot\|B\|$

**Третья матричная норма**
	$||A||_{\infty} = \max_{i} \sum \limits_{j = 1}^n |a_{ij}|$

# 3.	Согласованные матричные нормы.

**Матричная норма согласованная с векторной**
	Норма для которой выполняется неравенство $||Ax||\le||A||\cdot||x||\quad \forall x$

# 4.	Число обусловленности матрицы. Определение. Вычислительная формула. Пример плохо обусловленной системы.

**Число обусловленности матрицы $A$**.
	Также записывается как $\text{cond}(A)$
	Число, показывающее, насколько может измениться значение функции при небольшом изменении аргумента.
	$\mu(A) = \underset{ y \neq 0 }{ \underset{ x \neq 0 }{ sup } }\left(\dfrac{||Ax||}{||x||} / \dfrac{||Ay||}{||y||}\right)$
	$\mu(A) \geq 1$

**Вычислительная формула**
	$\mu(A) = ||A||\cdot ||A^{-1}||$

**Вывод**
	$\mu(A) = \underset{ y \neq 0 }{ \underset{ x \neq 0 }{ sup } }\left(\dfrac{||Ax||}{||x||} / \dfrac{||Ay||}{||y||}\right) = (\underset{ x \neq 0 }{ \sup }\dfrac{||Ax||}{||x||} / \underset{ y \neq 0 }{ \inf }\dfrac{||Ay||}{||y||})=$
	$z=Ay\implies y=A^{-1}z$
	$=\dfrac{||A||}{\inf\limits_{z\neq 0}\frac{||z||}{||A^{-1}z||}}=\dfrac{||A||}{\dfrac{1}{\sup\limits_{z\neq 0}\frac{||A^{-1}z||}{||z||}}}=\dfrac{||A||}{\frac{1}{||A^{-1}||}}=||A||\cdot ||A^{-1}||$

**Плохо обусловленная матрица**
	$A=\begin{pmatrix}\mathsf{1}&&\mathsf{1}\\\mathsf{1}&&\mathsf{1.01}\end{pmatrix}$
	$||A|| = 1 + 1.01$
	$||A^{-1}|| = \left|\left\|\begin{pmatrix}101 & -100 \\ -100 & 100\end{pmatrix}\right|\right| = 201$
	$||A|| \cdot ||A^{-1}|| = 404.01$
	Как видим, при небольшом изменении нашей начальной матрицы, мы получили большое изменение результата, что означает, что матрица плохо обусловлена

# 5.	Метод Гаусса.
## Описание метода
Пусть имеется некоторая система
$$
\begin{cases}
a_{11}x_{1} + \ldots + a_{1n}x_{n} = b_{1} \\
\cdots \\
a_{m_{1}}x_{1} + \ldots + a_{mn}x_{n} = b_{m}
\end{cases}
$$
С помощью элементарных преобразований приведём её к ступенчатому виду (aka прямой ход метода Гаусса)
$$
\begin{cases}
\alpha_{1j_{1}}x_{j_{1}} &  + \alpha_{1j_{2}}x_{j_{2}} &  + \ldots &  + \alpha_{1j_{r}}x_{j_r} &  + \ldots &  + \alpha_{1j_{n}}x_{j_{n}} &  = \beta_{1} \\
0 & +\alpha_{2j_{2}}x_{j_{2}} & +\ldots & +\alpha_{2j_{r}}x_{j_{r}} & +\ldots & + \alpha_{1j_{n}}x_{j_{n}} &  = \beta_{2} \\
\cdots \\
0 & +0 & +\ldots & +\alpha_{rj_{r}}x_{j_{r}} &  + \ldots & + \alpha_{rj_{n}}x_{j_{n}} &  = \beta_{r} \\
\ldots \\
0 & +0 & +\ldots & +0 & +\ldots & +0 &  = \beta_{r + 1} \\
\ldots \\
0 & +0 & +\ldots & +0 & +\ldots & +0 &  = \beta_{m}
\end{cases}
$$

Пусть $\forall i \geq r + 1 \quad \beta_{i} = 0$
Перенесём свободные переменные за знак равенств и поделим каждое из уравнений системы на свой коэффициент при самом левом $x$
$$
\begin{matrix} \\
\begin{cases}
x_{j_{1}} &  + \hat{ \alpha }_{1j_{2}}x_{j_{2}} &  + \ldots &  + \hat{ \alpha }_{1j_{r}} x_{j_{r}} &  = &  \hat{ \beta }_{1} &  - \hat{ \alpha }_{1j_{r + 1}} x_{j_{r + 1}} &  - \ldots &  - \hat{ \alpha }_{1j_{n}}x_{j_{n}} \\
0 & +x_{j_{2}} & +\ldots & +\hat{ \alpha }_{2j_{r}}x_{j_{r}} &  = & \hat{ \beta }_{2} &  - \hat{ \alpha }_{2j_{r + 1}}x_{j_{r + 1}} &  - \ldots & \hat{ -\alpha }_{2j_{n}}x_{j_{n}} \\
 &  &  &  & \cdots \\
0 &+0 &+0  &  +x_{j_{r}} & = & \hat{ \beta }_{r} & -\hat{ \alpha }_{rj_{r + 1}}x_{j_{r + 1}} &  - \ldots & -\hat{ \alpha }_{rj_{n}}x_{j_{n}}
\end{cases} \\
\hat{\beta_{i}} = \frac{\beta_{i}}{\alpha_{ij_{i}}} \\
\hat{ \alpha }_{ij_{k}} = \frac{\alpha_{ij_k}}{\alpha_{ij_{i}}} \\
i = 1, \ldots, r \\
k = i + 1, \ldots, n 
\end{matrix}
$$


**Замечание**
	После прямого хода метода Гаусса можно легко вычислить определитель $|A|=\prod\limits_{i=1}^{n}a_{ii}$
	(Лол)

# 6.	LU – разложение. Необходимое и достаточное условие существования.
**LU - разложение**
	Декомпозиция матрицы системы $A$ на две треугольные матрицы $L, U$ так, что бы $A=L\cdot U$
	$$A=
	\begin{pmatrix}
	{a_{11}} && {a_{12}} && {\ldots} && {a_{1n}} \\
	{a_{21}} && {a_{22}} && {\ldots} && {a_{2n}} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{a_{n1}} && {a_{n2}} && {\ldots} && {a_{nn}} 
	\end{pmatrix}$$
	$$
	\quad L =
	\begin{pmatrix}
	{1} && {0} && {\ldots} && {0} \\
	{l_{21}} && {1} && {\ldots} && {0} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{l_{n1}} && {l_{n2}} && {\ldots} && {1} 
	\end{pmatrix}
	\quad U=
	\begin{pmatrix}
	{u_{11}} && {u_{12}} && {\ldots} && {u_{1n}} \\
	{0} && {u_{22}} && {\ldots} && {u_{2n}} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{0} && {0} && {\ldots} && {u_{nn}} 
	\end{pmatrix} 
	$$
	Дальше по правилам умножения матриц выводятся общие формулы элементов
	$l_{ii} = 1$
	*Если $i\ge j:$
		$u_{ij}=a_{ij}-\sum\limits_{k=1}^{i} l_{ik}\cdot u_{kj}$
	Если $i<j:$*
		$l_{ij}=\dfrac{a_{ij}-\sum\limits_{k=1}^j l_{ik}\cdot u_{kj} }{u_{jj}}$

**Вывод формул (скорее всего, потребуется нижний вариант)**
	$\sum \limits_{ j=1 }^{ n } l_{ij}u_{jk} = a_{ik} \quad i, k = \overline{  1, \ldots, n }$
	
	$i < j \implies l_{ij} = 0 \quad j > k \implies u_{jk} = 0$
	Всего у нас $n^{2}$ уравнений с $\underbrace{ \frac{1}{2}n(n + 1) }_{ l_{ij} } + \underbrace{ \frac{1}{2}n(n - 1) }_{ u_{jk} } = n^{2}$ неизвестными
	$$
	\begin{cases}
	\sum \limits_{ j=1 }^{ k } l_{ij}u_{jk} = a_{ik} & k \le i \\
    \sum \limits_{ j=1 }^{ i } l_{ij}u_{jk} = a_{ik} & k > i 
	\end{cases}
	$$
	Выделим случаи с $k = 1$ и $i = 1$. Также учтём, что $u_{kk} = 1$
	$$
     \begin{cases}
     \begin{cases}
     l_{i1} = a_{i{1}} & i = 1, \ldots, n \\
     \sum \limits_{ j=1 }^{ k - 1 } l_{ij}u_{jk} + l_{ik} = a_{ik} & 1 < k \leq i, \quad i, k = 2, \ldots, n  
     \end{cases} \\
     \begin{cases}
     l_{11} = u_{1k} = a_{1k} & k = 2, \ldots, n \\
     \sum \limits_{ j=1 }^{ i - 1 } l_{ij}u_{jk} + l_{ik} = a_{ik} & k > i > 1, \quad i, k = 2, \ldots, n 
     \end{cases}
     \end{cases}
    $$
    $$
     \begin{cases}
     \begin{cases}
     l_{i1} = a_{i1} & i = 1, \ldots, n \\
     u_{1k} = \frac{a_{1k}}{l_{11}} & k = 2, \ldots, n
     \end{cases} \\
     \begin{cases}
     l_{ik} = a_{ik} - \sum \limits_{ j = 1 }^{ k - 1 } l_{ij}u_{jk} & 1 < k \leq i, \quad i, k = 2, \ldots, n \\
     u_{ik} = \frac{1}{l_{ii}}\left( a_{ik} - \sum \limits_{ j=1 }^{ i - 1 } l_{ij}u_{jk} \right)  & k > i > 1, \quad i, k = 2, \ldots, n  
     \end{cases}
     \end{cases}
     $$
**Вывод формулы через Гаусса**
	Нужен, чтобы показать существование таких матриц (и зависимость метода от Гаусса, лол)
	Возьмём матрицы $D_{i}$, такие, что $d_{ij}$ являются множителями Гаусса ($d_{21} = \frac{a_{21}}{a_{11}}$) и т. д.
	$$
	D_{1} = \begin{pmatrix}
	1 & 0 & \ldots & 0\\
	-d_{21} & 1 & \ldots & 0 \\
	\ldots & \ldots & \ldots & \ldots \\
	-d_{n1} & 0 & \ldots & 1
	\end{pmatrix}  \quad 
	
	D_{2} = \begin{pmatrix}
	1 & 0 & \ldots & 0\\
	0 & 1 & \ldots & 0 \\
	\ldots & \ldots & \ldots & \ldots \\
	0 & -d_{n2} & \ldots & 1
	\end{pmatrix}  \quad 
	\cdots  \quad  
	D_{n - 1}
	$$
	$$A_i=D_iA_{i-1}$$
	$$U=A_{n-1}=D_{n-1}A_{n-2}=...=D_{n-1}D_{n-2}...D_1A=DA$$
	$$D^{-1}=L$$
	$$A=LU$$
	$$
	L = \begin{pmatrix}
	1 & 0 & 0 & \ldots & 0 \\
	d_{21} & 1 & 0  & \ldots & 0 \\
	d_{21}d_{32} + d_{31} & d_{32} & 1 & \ldots & 0 \\
	\ldots & \ldots & \ldots & \ldots & \ldots \\
	l_{n1} & l_{n2} & l_{n3} & \ldots & 1
	\end{pmatrix}
	$$
	У нас получится нижнедиагональная матрица. Просто переобозначьте суммы коэффициентов на $l_{ij}$ и вс, лол

**Необходимое и достаточное условие существования**
	Все угловые миноры матрицы $A$ невырожденные

**Замечание**
	После LU разложения можно легко вычислить определитель $|A|=\prod\limits_{i=1}^{n}u_{ii}$

**Применение**
	Предположим, у нас есть система $Ax = b$
	Тогда, $Ly  = b, \quad Ux = y$
	Из этих уравнений можно проще высчитать наши $x$.
# 7.	Метод квадратного корня (a.k.a Метод Холецкого).
**Метод квадратного корня**
	Частный случай $LU$ разложения для симметричных матриц $(A=A^T)$
		$A = \begin{pmatrix}a_{11} & a_{12} & \ldots & a_{1n} \\ a_{12} & a_{22} & \ldots & a_{2n} \\ \vdots & \vdots & \ddots & \cdots \\ a_{1n} & a_{2n} & \ldots & a_{nn} \end{pmatrix}$
	$A=LL^{T}$ или $A = U^{T}U$
	Разберём на примере $A = LL^T$
	$l_{11} = \sqrt{ a_{11} }$
	$l_{j1} = \frac{a_{j1}}{l_{11}}$
	$l_{ii} = \sqrt{ a_{ii} - \underset{ p = 1 }{ \overset{ i - 1 }{ \sum } }l^{2}_{ip} }, \quad i \in \left[ 2, n \right]$
	$l_{ji} = \frac{1}{l_{ii}}\left( a_{ji} - \underset{ p = 1 }{ \overset{ i - 1 }{ \sum } }l_{ip}l_{jp} \right), \quad i \in \left[ 2, n - 1 \right],\ j \in \left[ i + 1, n \right]$

**Вывод формул**
		$A=LU\quad U = DV$
		$D = \begin{pmatrix}d_1 & 0 & \ldots & 0 \\ 0 & d_2 & \ldots & 0 \\ \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & \ldots & d_n\end{pmatrix}\quad V = \begin{pmatrix}1 & v_{12} & \ldots & v_{1n} \\ 0 & 1 & \ldots & v_{2n} \\ \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & \ldots & 1\end{pmatrix}$
		$d_{i} = u_{ii}\quad v_{ij} = \dfrac{u_{ij}}{u_{ii}}$
		
		$A = LDV$
		$A^{T} = (LDV)^{T} = V^{T}D^{T}L^{T} = V^{T}DL^{T}=A$
		$L = V^{T}\quad V = L^{T}$
		$A = V^{T}DV$
		$D^{1/2} = \begin{pmatrix}\sqrt{d_1} & 0 & \ldots & 0 \\ 0 & \sqrt{d_2} & \ldots & 0 \\ \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & \ldots & \sqrt{d_n}\end{pmatrix}$
		$A = V^{T}D^{1/2}D^{1/2}V$
		$A = W^{T}W\quad W=D^{1\backslash 2}V$
		
		$a_{ij} = \sum \limits_{ k = 1 } ^{ n } w^{T}_{ik}w_{kj} = \sum \limits_{ k = 1 } ^{ i }w_{ki}w_{kj} = a_{ij}$
		$a_{11} = w_{11} w_{11}$
		$w_{11} = \sqrt{ a_{11 }}$
		$a_{1j} = w_{11}w_{1j}  \quad   \quad w_{1j} = \dfrac{a_{1j}}{w_{11}}$
		
		$w_{ii} = \sqrt{ a_{ii} - \sum \limits_{ k=1 }^{ i - 1 }w_{ki}^{2} }$
		$w_{ij} = \dfrac{a_{ii} - \sum \limits_{ k=1 }^{ i - 1 }w_{ki}w_{kj}}{w_{ii}}  \quad   \quad j = i + 1, \ldots, n$

	

# 8.	Матрица отражения, ее свойства.
**Определение**
	Пусть $p\in \mathbb{R}^n-$ вектор нормали некоторой гиперплоскости в $\mathbb{R}^n$, проходящей через $\theta$
	Тогда $y=x-2\dfrac{(p, x)}{(p, p)}p~-$ задает вектор $y$, отраженный к $x$ относительно гиперплоскости $(p, x)=0$
	Проведем ряд преобразований:
	$y=x-2\dfrac{(p, x)}{(p, p)}p=x-\dfrac{2}{(p, p)}\cdot(p,x)\cdot p=x-\dfrac{2}{(p, p)}\cdot p \cdot p^T \cdot x=(E-\dfrac{2}{(p, p)}\cdot p \cdot p^T)\cdot x$
	Обозначим $P=E-\dfrac{2}{(p, p)}\cdot p \cdot p^T$
	Тогда $P~-$ матрица отражения

**Свойства**
1. $P^2=E$
2. $P^T=P$
3. $P^T\cdot P = P^2=E\implies P-$ ортогональная матрица
4. При замене нормали $p=\beta\cdot p$ матрица отражения не изменится
5. $p=x-y$
6. $p_i=0\implies y_i = x_i \quad i=\overline{1,k}$
7. $\begin{cases}{p_1=\ldots=p_k=0}\\ {x_{k+1}=\ldots=x_n=0}\end{cases}\implies y_{k+1}=\ldots=y_n=0$

# 9.	QR – разложение. Условие существования QR – разложения.
**$QR$ разложение**
	$A = QR$
	$A_{1} = P_{1}A$
		$P_{1}$ - матрица отражения, такая, чтобы $1$-й столбец матрицы имел вид:
	$\begin{pmatrix}a_{11}^{(1)}\\ 0\\ \ldots\\ 0\end{pmatrix} = a_{1}^{(1)}  \quad \begin{pmatrix}a_{11}\\ a_{21}\\ \ldots\\ a_{n1}\end{pmatrix} = a_{1}$ 
	
	$P_{1}a_{1} = \begin{pmatrix}a_{11}^{(1)}\\ 0\\ \ldots\\ 0\end{pmatrix}$
	
	$p_{1}^{(1)} = a_{11} - a_{11}^{(1)}$
	$p_{2}^{(1)} = a_{21}$
	$\ldots$
	$p_{n}^{(1)} = a_{n{1}}$
	
	$||a_{1}^{(1)}||_{2}^{2} = ||a_{1}||_{2}^{2}$
	$(a_{11}^{(1)})^{2} = \sum \limits_{ l=1 }^{ n }a_{l1}^{2}$
	$a_{11}^{(1)} = \pm \sqrt{ \sum \limits_{ l=1 }^{ n }a_{l1}^{2} }$
	$p_{1}^{(1)} = a_{11} \pm \sqrt{ \sum \limits_{ l=1 }^{ n }a_{l1}^{2} }$
	
	$\sigma_{1} = \begin{cases}1, & a_{11}\geq 0\\ -1, & a_{11}< 0 \end{cases}$
	
	$p_{1}^{(1)} = a_{11} + \sigma_{1} \cdot \sqrt{ \sum \limits_{ l=1 }^{ n }a_{l{1}}^{2} }$
	
	$a_{j}^{(1)} = a_{j} - 2 \dfrac{(p^{(1)}, a_{j})}{(p^{(1)}, p^{(1)})}p^{(1)}  \quad j = 2, \ldots, n$
	**Рассмотрим k-ый шаг**
	*Из-за отображения, у нас меняются все элементы*
	$A_{k} = P_{k}A_{k - 1}$
	$a_{k}^{(k)} = P_{k}a_{k - 1}^{(k - 1)} = \begin{pmatrix}a_{1k}^{(k)} \\ a_{2k}^{(k)} \\ \vdots \\ a_{k - 1, k}^{(k)} \\  a_{kk}^{(k)} \\ 0 \\ \vdots \\ 0\end{pmatrix}$
	$p^{(k)} = a_{k}^{(k - 1)} - a_{k}^{(k)}$
	$p^{(k)}_{l} = 0, \quad l = 1, 2, \ldots, n - 1$
	$p^{(k)}_{k} = a_{kk}^{(k - 1)} - a_{kk}^{(k)}$
	$p_{l}^{(k)} = a_{lk}^{(k - 1)}, \quad l = k + 1, \ldots, n$
	$||a_{k}^{(k)}||_{2}^{2} = ||a_{k}^{(k - 1)}||_{2}^{2}$
	$a_{kk}^{(k)} = \pm \sqrt{ \sum \limits_{ l=k }^{ n } \left( a_{lk}^{(k - 1)} \right)^{2} }$
	$\sigma_{k} = \begin{cases}1, & a_{kk}^{(k - 1)} \geq 0 \\ -1, & a_{kk}^{(k - 1)} < 0 \end{cases}$
	$p_{k}^{(k)} = a_{kk}^{(k - 1)} + \sigma_{k}\sqrt{ \sum \limits_{ l=k }^{ n }\left( a_{lk}^{(k - 1)} \right)^{2} }$
	$a_{j}^{(k)} = a_{j}^{(k - 1)} - 2\dfrac{\left( p^{(k)}, a_{j}^{(k - 1)} \right)}{\left( p^{(k)}, p^{(k)} \right)}p^{k}$
	$a_{ij}^{(k)} = a_{ij}^{(k - 1)} - 2 \dfrac{p_{i}^{(k)}}{\sum \limits_{ l=k }^{ n }\left( p_{l}^{(k)} \right)^{2}}\sum \limits_{ l=k }^{ n }p_{l}^{(k)}a_{lj}^{(k - 1)} \quad j = k + 1, \ldots, n$
	 $p_{1}^{(k)} = p_{2}^{(k)} = \ldots = p_{k - 1}^{(k)} = 0$ ( Коэффициенты  до $k$ будут нулевыми)
	**По прошествии $n - 1$ шага получим матрицу $R$**
	$R = A_{n - 1} = P_{n - 1}A_{n - 2} = P_{n - 1}P_{n - 2}A_{n - 3} = \ldots = P_{n - 1}P_{n - 2} \ldots P_{2}P_{1}A$
	$Q = P_{1}P_{2}\ldots P_{n - 1}$
	Матрица $Q$ будет ортогональной (тк является результатом произведения ортогональных матриц)
	$R = Q^{T}A$
	$A = QR$
	**Рассмотрим случай деления на 0**
	$\sum \limits_{ l=k }^{ n }\left( p_{l}^{(k)} \right)^{2} = 0 \iff p_{l}^{(k)} = 0, \quad l = k, \ldots, n$
	Подберём вектор нормали, который будет ненулевым, но оставит вектор на месте.
	$p^{(k)} = \begin{pmatrix}0 \\ \vdots \\ 0 \\ \sqrt{ 2 } \\ 0 \\ \vdots \\ 0\end{pmatrix}$
**Условие существования**
	Разложение есть всегда(если матрица невырожденая)

# 10.	Метод окаймления обращения матриц.
**Метод окаймления**
	Нужен для нахождения обратной матрицы
$A_j-$ матрица порядка $j$
Метод вычисления матрицы $A_i^{-1},$ при известной $A_{i-1}^{-1}$
$$A_i = \begin{pmatrix}
{A_{i-1}} && {a_{12}} \\
{a_{21}} && {a_{22}}
\end{pmatrix}
\quad
A_i^{-1} = \begin{pmatrix}
{B_{i-1}} && {b_{12}} \\
{b_{21}} && {b_{22}}
\end{pmatrix}$$
$a_{12}, b_{12}-$ вектор столбец, $a_{21}, b_{21}-$ вектор строка, $a_{22}, b_{22}-$ числа
$b_{22} = \frac{1}{k}$
По определению обратной матрицы:
	$A_{i}\cdot A_{i}^{-1}=\begin{pmatrix} {E} && {\theta}\\ {\theta^T} && {1} \end{pmatrix}$
По правилам перемножения матриц получаем систему:
	$$
	\begin{cases}
	{A_{i-1}\cdot B_{i-1}+a_{12}\cdot b_{21}=E} \\
	{A_{i-1}\cdot b_{12}+a_{12}\cdot b_{22}=\theta} \\
	{a_{21}\cdot B_{i - 1}+a_{22}\cdot b_{21}=\theta^T} \\
	{a_{22}\cdot b_{22} + a_{21}\cdot b_{12}=1}
	\end{cases}
	$$
$$
\begin{matrix}
{} \\
\begin{cases}
B_{i - 1} = A^{-1}_{n - 1} - A^{-1}_{n - 1}a_{12}b_{21} \\
b_{12} = -\frac{1}{k}A^{-1}_{n - 1}a_{12} \\
\frac{1}{b_{22}} = k =a_{22}-a_{21}\cdot A_{i-1}^{-1}\cdot a_{12} \\
b_{21} = -\frac{1}{k}a_{12}A^{-1}_{n - 1}
\end{cases}
\end{matrix}
$$

Таким образом,
$A_{i}^{-1} = \begin{pmatrix}A_{n - 1}^{-1} + \frac{1}{k}A_{n - 1}A^{-1}a_{12}a_{21}A_{n - 1}^{-1} & -\frac{1}{k}A_{n - 1}^{-1}a_{12} \\ -\frac{1}{k}a_{21}A^{-1}_{n - 1} & \frac{1}{k}\end{pmatrix}$
Где $k=a_{22}-a_{21}\cdot A_{i-1}^{-1}\cdot a_{12}$
