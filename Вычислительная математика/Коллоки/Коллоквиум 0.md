
# 1.	Подчиненные матричные нормы. Свойства. Формула для вычисления второй нормы матрицы.

**Подчинённая матричная норма**
	Норма, которая вычисляется по формуле $\|A\| = \underset{x \neq 0}{\sup} \dfrac{\|A{x}\|}{\|x\|}$
**Свойства**
1. $\|E\| = 1$
		$\| E \|=\underset{x \neq 0}{\sup} \dfrac{\|E{x}\|}{\|x\|} = \underset{x \neq 0}{\sup} \dfrac{\|x\|}{\|x\|}=1$
2. $\|AB\| \le \|A\| \cdot \|B\|$
		$\|AB\| = \underset{x \neq 0}{\sup} \dfrac{\|AB{x}\|}{\|x\|}\le \underset{x \neq 0}{\sup} \dfrac{\|A\|\cdot\|B{x}\|}{\|x\|}=\|A\|\cdot\|B\|$

**Вторая матричная норма**
	$||A||_{2} = \sqrt{\max\limits_{i} \lambda_{i}(A^T A)}$

# 2.	Подчиненные матричные нормы. Свойства. Формула для вычисления третьей нормы матрицы.

**Подчинённая матричная норма**
	Норма, которая удовлетворяет условию $||A|| = \underset{x \neq 0}{\sup} \dfrac{||A{x}||}{||x||}$
	То есть, если матричная норма подчинена введённой до неё векторной норме
	Любая подчинённая матричная норма согласованна. Обратное — не факт.
	*Подчинение - более сильная форма согласования*
**Свойства**
1. $\|E\| = 1$
		$\| E \|=\underset{x \neq 0}{\sup} \dfrac{\|E{x}\|}{\|x\|} = \underset{x \neq 0}{\sup} \dfrac{\|x\|}{\|x\|}=1$
2. $\|AB\| \le \|A\| \cdot \|B\|$
		$\|AB\| = \underset{x \neq 0}{\sup} \dfrac{\|AB{x}\|}{\|x\|}\le \underset{x \neq 0}{\sup} \dfrac{\|A\|\cdot\|B{x}\|}{\|x\|}=\|A\|\cdot\|B\|$

**Третья матричная норма**
	$||A||_{\infty} = \max_{i} \sum \limits_{j = 1}^n |a_{ij}|$

# 3.	Согласованные матричные нормы.

**Матричная норма согласованная с векторной**
	Норма для которой выполняется неравенство $||Ax||\le||A||\cdot||x||\quad \forall x$

# 4.	Число обусловленности матрицы. Определение. Вычислительная формула. Пример плохо обусловленной системы.

**Число обусловленности матрицы $A$**.
	Также записывается как $\text{cond}(A)$
	Число, показывающее, насколько может измениться значение функции при небольшом изменении аргумента.
	$\mu(A) = \underset{ y \neq 0 }{ \underset{ x \neq 0 }{ sup } }\left(\dfrac{||Ax||}{||x||} / \dfrac{||Ay||}{||y||}\right)$

**Вычислительная формула**
	$\mu(A) = ||A||\cdot ||A^{-1}||$

**Вывод**
	$\mu(A) = \underset{ y \neq 0 }{ \underset{ x \neq 0 }{ sup } }\left(\dfrac{||Ax||}{||x||} / \dfrac{||Ay||}{||y||}\right) = (\underset{ x \neq 0 }{ \sup }\dfrac{||Ax||}{||x||} / \underset{ y \neq 0 }{ \inf }\dfrac{||Ay||}{||y||})=$
	$z=Ay\implies y=A^{-1}z$
	$=\dfrac{||A||}{\inf\limits_{z\neq 0}\frac{||z||}{||A^{-1}z||}}=\dfrac{||A||}{\dfrac{1}{\sup\limits_{z\neq 0}\frac{||A^{-1}z||}{||z||}}}=\dfrac{||A||}{\frac{1}{||A^{-1}||}}=||A||\cdot ||A^{-1}||$

**Плохо обусловленная матрица**
	$A=\begin{pmatrix}\mathsf{1}&&\mathsf{1}\\\mathsf{1}&&\mathsf{1.01}\end{pmatrix}$
	$||A|| = 1 + 1.01$
	$||A^{-1}|| = \left|\left\|\begin{pmatrix}101 & -100 \\ -100 & 100\end{pmatrix}\right|\right|$
	Оставлю вычисление $\mathsf{cond(A)}$ читателю в качестве несложного упражнения
	P. S. Там что-то около 400

# 5.	Метод Гаусса.
## Описание метода
Пусть имеется некоторая система
$$
\begin{cases}
a_{11}x_{1} + \ldots + a_{1n}x_{n} = b_{1} \\
\cdots \\
a_{m_{1}}x_{1} + \ldots + a_{mn}x_{n} = b_{m}
\end{cases}
$$
С помощью элементарных преобразований приведём её к ступенчатому виду (aka прямой ход метода Гаусса)
$$
\begin{cases}
\alpha_{1j_{1}}x_{j_{1}} &  + \alpha_{1j_{2}}x_{j_{2}} &  + \ldots &  + \alpha_{1j_{r}}x_{j_r} &  + \ldots &  + \alpha_{1j_{n}}x_{j_{n}} &  = \beta_{1} \\
0 & +\alpha_{2j_{2}}x_{j_{2}} & +\ldots & +\alpha_{2j_{r}}x_{j_{r}} & +\ldots & + \alpha_{1j_{n}}x_{j_{n}} &  = \beta_{2} \\
\cdots \\
0 & +0 & +\ldots & +\alpha_{rj_{r}}x_{j_{r}} &  + \ldots & + \alpha_{rj_{n}}x_{j_{n}} &  = \beta_{r} \\
\ldots \\
0 & +0 & +\ldots & +0 & +\ldots & +0 &  = \beta_{r + 1} \\
\ldots \\
0 & +0 & +\ldots & +0 & +\ldots & +0 &  = \beta_{m}
\end{cases}
$$

Пусть $\forall i \geq r + 1 \quad \beta_{i} = 0$
Перенесём свободные переменные за знак равенств и поделим каждое из уравнений системы на свой коэффициент при самом левом $x$
$$
\begin{matrix} \\
\begin{cases}
x_{j_{1}} &  + \hat{ \alpha }_{1j_{2}}x_{j_{2}} &  + \ldots &  + \hat{ \alpha }_{1j_{r}} x_{j_{r}} &  = &  \hat{ \beta }_{1} &  - \hat{ \alpha }_{1j_{r + 1}} x_{j_{r + 1}} &  - \ldots &  - \hat{ \alpha }_{1j_{n}}x_{j_{n}} \\
0 & +x_{j_{2}} & +\ldots & +\hat{ \alpha }_{2j_{r}}x_{j_{r}} &  = & \hat{ \beta }_{2} &  - \hat{ \alpha }_{2j_{r + 1}}x_{j_{r + 1}} &  - \ldots & \hat{ -\alpha }_{2j_{n}}x_{j_{n}} \\
 &  &  &  & \cdots \\
0 &+0 &+0  &  +x_{j_{r}} & = & \hat{ \beta }_{r} & -\hat{ \alpha }_{rj_{r + 1}}x_{j_{r + 1}} &  - \ldots & -\hat{ \alpha }_{rj_{n}}x_{j_{n}}
\end{cases} \\
\hat{\beta_{i}} = \frac{\beta_{i}}{\alpha_{ij_{i}}} \\
\hat{ \alpha }_{ij_{k}} = \frac{\alpha_{ij_k}}{\alpha_{ij_{i}}} \\
i = 1, \ldots, r \\
k = i + 1, \ldots, n 
\end{matrix}
$$


**Замечание**
	После прямого хода метода Гаусса можно легко вычислить определитель $|A|=\prod\limits_{i=1}^{n}a_{ii}$

# 6.	LU – разложение. Необходимое и достаточное условие существования.

**LU - разложение**
	Декомпозиция матрицы системы $A$ на две треугольные матрицы $L, U$ так, что бы $A=L\cdot U$
	$$A=
	\begin{pmatrix}
	{a_{11}} && {a_{12}} && {\ldots} && {a_{1n}} \\
	{a_{21}} && {a_{22}} && {\ldots} && {a_{2n}} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{a_{n1}} && {a_{n2}} && {\ldots} && {a_{nn}} 
	\end{pmatrix}$$
	$$
	\quad L =
	\begin{pmatrix}
	{1} && {0} && {\ldots} && {0} \\
	{l_{21}} && {1} && {\ldots} && {0} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{l_{n1}} && {l_{n2}} && {\ldots} && {1} 
	\end{pmatrix}
	\quad U=
	\begin{pmatrix}
	{u_{11}} && {u_{12}} && {\ldots} && {u_{1n}} \\
	{0} && {u_{22}} && {\ldots} && {u_{2n}} \\
	{\ldots} && {\ldots} && {\ldots} && {\ldots} \\
	{0} && {0} && {\ldots} && {u_{nn}} 
	\end{pmatrix} 
	$$
	Дальше по правилам умножения матриц выводятся общие формулы элементов
	*Если $i\ge j:$
		$u_{ij}=a_{ij}-\sum\limits_{k=1}^{i} l_{ik}\cdot u_{kj}$
	Если $i<j:$*
		$l_{ij}=\dfrac{a_{ij}-\sum\limits_{k=1}^j l_{ik}\cdot u_{kj} }{u_{jj}}$

**Необходимое и достаточное условие существования**
	Все угловые миноры матрицы $A$ невырожденные

**Замечание**
	После LU разложения можно легко вычислить определитель $|A|=\prod\limits_{i=1}^{n}u_{ii}$

# 7.	Метод квадратного корня.
**Метод квадратного корня**
	Частный случай $LU$ разложения для симметричных матриц $(A=A^T)$
	$A=LU\quad U = DV$
	$D = \begin{pmatrix}d_1 & 0 & \ldots & 0 \\ 0 & d_2 & \ldots & 0 \\ \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & \ldots & d_n\end{pmatrix}\quad v = \begin{pmatrix}1 & v_{12} & \ldots & v_{1n} \\ 0 & 1 & \ldots & v_{2n} \\ \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & \ldots & 1\end{pmatrix}$
	$d_{i} = u_{ii}\quad v_{ij} = \dfrac{u_{ij}}{u_{ii}}$
	$A = LDV$
	$A^{T} = (LDV)^{T} = V^{T}D^{T}L^{T} = V^{T}DL^{T}=A$
	$L = V^{T}\quad V = L^{T}$
	$A = V^{T}DV$
	$D^{1/2} = \begin{pmatrix}\sqrt{d_1} & 0 & \ldots & 0 \\ 0 & \sqrt{d_2} & \ldots & 0 \\ \ldots & \ldots & \ldots & \ldots \\ 0 & 0 & \ldots & \sqrt{d_n}\end{pmatrix}$
	$A = V^{T}D^{1/2}D^{1/2}V$
	$A = W^{T}W\quad W=D^{1\backslash 2}V$
	
	$a_{ij} = \sum \limits_{ k = 1 } ^{ n } w^{T}_{ik}w_{kj} = \sum \limits_{ k = 1 } ^{ i }w_{ki}w_{kj} = a_{ij}$
	$a_{11} = w_{11} w_{11}$
	$w_{11} = \sqrt{ a_{11 }}$
	$a_{1j} = w_{11}w_{1j}  \quad   \quad w_{1j} = \dfrac{a_{1j}}{w_{11}}$
	
	$w_{ii} = \sqrt{ a_{ii} - \sum \limits_{ k=1 }^{ i - 1 }w_{ki}^{2} }$
	$w_{ij} = \dfrac{a_{ii} - \sum \limits_{ k=1 }^{ i - 1 }w_{ki}w_{kj}}{w_{ii}}  \quad   \quad j = i + 1, \ldots, n$

# 8.	Матрица отражения, ее свойства.
**Определение**
	Пусть $p\in R^n-$ вектор нормали некоторой гиперплоскости в $R^n$, проходящей через $\theta$
	Тогда $y=x-2\dfrac{(p, x)}{(p, p)}p~-$ задает вектор $y$, отраженный к $x$ относительно гиперплоскости $(p, x)=0$
	Проведем ряд преобразований:
	$y=x-2\dfrac{(p, x)}{(p, p)}p=x-\dfrac{2}{(p, p)}\cdot(p,x)\cdot p=x-\dfrac{2}{(p, p)}\cdot p \cdot p^T \cdot x=(E-\dfrac{2}{(p, p)}\cdot p \cdot p^T)\cdot x$
	Обозначим $P=E-\dfrac{2}{(p, p)}\cdot p \cdot p^T$
	Тогда $P~-$ матрица отражения

**Свойства**
1. $P^2=E$
2. $P^T=P$
3. $P^T\cdot P = P^2=E\implies P-$ ортогональная матрица
4. При замене нормали $p=\beta\cdot p$ матрица отражения не изменится
5. $p=x-y$
6. $p_i=0\implies y_i = x_i \quad i=\overline{1,k}$
7. $\begin{cases}{p_1=\ldots=p_k=0}\\ {x_{k+1}=\ldots=x_n=0}\end{cases}\implies y_{k+1}=\ldots=y_n=0$

# 9.	QR – разложение. Условие существования QR – разложения.



**Условие существования**
	Разложение есть всегда(если матрица невырожденая)

# 10.	Метод окаймления обращения матриц.
А это надо гуглить
Юху!
Буду сегодня перед сном смотреть на матрицы...
